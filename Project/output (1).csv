dtstart,dtend,dtstamp,organizer,uid,attendee,created,description,last-modified,location,sequence,status,summary,transp
"2022-05-18T02:00:00Z","2022-05-18T02:04:00Z","2024-03-21T14:46:32Z","","6dbef136-eb5b-47eb-870f-ebd51255d874@conf.researchr.org","","2022-05-03T15:38:48Z","GitHub and OpenAI recently launched GitHub Copilot, an “AI pair programmer” that utilizes the power of Natural Language Processing, Static Analysis, Code Synthesis, and Artificial Intelligence. Given a natural language description of the target functionality, Copilot can generate corresponding code in several programming languages. In this paper, we perform an empirical study to understand the correctness and understandability of the Copilot’s suggested code. We use 33 LeetCode questions to create queries for Copilot in four different programming languages. We evaluate the correctness of the corresponding 132 Copilot solutions by running LeetCode’s provided tests, and evaluate understandability using SonarQube’s cyclomatic complexity and cognitive complexity metrics. We find that Copilot’s Java suggestions have the highest correctness score (57%) while JavaScript is lowest (27%). Overall, Copilot’s suggestions have low complexity with no notable differences between the programming languages. We also find some potential Copilot shortcomings, such as generating code that can be further simplified and code that relies on undefined helper methods.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] An Empirical Evaluation of GitHub Copilot’s Code Suggestions - Nhan Nguyen, Sarah Nadi",""
"2022-05-18T02:04:00Z","2022-05-18T02:08:00Z","2024-03-21T14:46:32Z","","9a5d2b25-3d82-4975-af60-8933d613d684@conf.researchr.org","","2022-05-03T15:37:55Z","An important function of code review is to increase understanding; helping reviewers understand a code change aides in knowledge transfer and finding bugs. Comments in code largely serve a similar purpose, helping future readers understand the program. It is thus natural to study what happens when these two forms of understanding collide. We ask: what documentation-related comments do reviewers make and how do they affect understanding of the contribution? We analyze ca.~700K review comments on 2,000 (Java and Python) GitHub projects, and propose several filters to identify which comments are likely to be either in response to a change in documentation and/or a call for such a change. We identify 65K such cases. We next develop a taxonomy of the reviewer intents behind such “comments on comments”. We find that achieving a shared understanding of the code is key: reviewer comments most often focused on clarification, followed by pointing out issues to fix, such as typos and outdated comments. Curiously, clarifying comments were frequently suggested (often verbatim) by the reviewer, indicating a desire to persist their understanding acquired during code review. We conclude with a discussion of implications of our comments-on-comments dataset for research on improving code review, including the potential benefits for automating code review.","2022-05-14T14:17:12Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Comments on Comments: Where Code Review and Documentation Meet - Nikitha Rao, Jason Tsay, Martin Hirzel, Vincent J. Hellendoorn",""
"2022-05-18T02:08:00Z","2022-05-18T02:15:00Z","2024-03-21T14:46:32Z","","a98833b7-0b88-4116-b6af-99d1ebb002b9@conf.researchr.org","","2022-05-03T15:37:55Z","Stack Overflow has become an essential technical resource for developers. However, given the vast amount of knowledge available on Stack Overflow, finding the right information that is relevant for a given task is still challenging, especially when a developer is looking for a solution that applies to their specific requirements or technology stack. Clearly marking answers with their \\textit{context}, i.e., the information that characterizes the technologies and assumptions needed for this answer, is potentially one way to improve navigation. However, there is no information about how often such context is mentioned, and what kind of information it might offer. In this paper, we conduct an empirical study to understand the occurrence of technical context in Stack Overflow answers and comments, using tags as a proxy for technical context. We specifically focus on \\textit{additional context}, where answers/comments mention information that is not already discussed in the question. Our results show that nearly half of our studied threads contain at least one additional context. We find that almost 50% of the additional context are either a library/framework, a programming language, a tool/application, an API, or a database. We also find that answer votes alone are not a good indicator of the presence of additional context in an answer, which suggests that further visual cues to aid navigation may be needed. Overall, our findings show the promise of using additional context as navigational cues.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Does This Apply to Me? An Empirical Study of Technical Context in Stack Overflow - Akalanka Galappaththi, Sarah Nadi, Christoph Treude",""
"2022-05-18T02:15:00Z","2022-05-18T02:22:00Z","2024-03-21T14:46:32Z","","3e8a24f7-9235-4c13-9a53-edd8821f7d0a@conf.researchr.org","","2022-05-03T15:38:48Z","In agile iterative development, an agile team needs to analyze documented information for effort estimation and sprint planning. While documentation can be changed, the documentation changes after sprint planning may invalidate the estimated effort and sprint plan. Hence, to help the team be aware of the potential documentation changes, we developed DocWarn to estimate the probability that a work item will have documentation changes. We developed three variations of DocWarn, which are based on the characteristics extracted from the work items (DocWarn-C), the natural language text (DocWarn-T), and both inputs (DocWarn-H). \nBased on nine open-source projects that work in sprints and actively maintain documentation, DocWarn can predict the documentation changes with an average AUC of 0.75 and an average F1-Score of 0.36, which are significantly higher than the baselines. We also found that the most influential characteristics of a work item for determining the future documentation changes are the past tendency of developers and the length of description text. Based on the qualitative assessment, we found that 40%-68% of the correctly predicted documentation changes were related to scope modification. With the prediction of DocWarn, the team will be better aware of the potential documentation changes during sprint planning, allowing the team to manage the uncertainty and reduce the risk of unreliable effort estimation and sprint planning.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Towards Reliable Agile Iterative Planning via Predicting Documentation Changes of Work Items - Jirat Pasuksmit, Patanamon Thongtanunam, Shanika Karunasekera",""
"2022-05-18T02:22:00Z","2022-05-18T02:29:00Z","2024-03-21T14:46:32Z","","5fd2834e-9c3b-49c9-83f8-316a5119f835@conf.researchr.org","","2022-05-03T15:37:55Z","Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort on practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository, and disregarding features that were shown to be effective in other domains. To address this gap, we propose using a machine learning based approach to identify the bot accounts regardless of their activity level. We extracted 19 features related to the account’s profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) are the most important features to identify the account type. Finally, we compare the performance of the Random Forest classifier to the state-of-the-art approaches, and our results show that our Random Forest model outperforms the state-of-the-art techniques in identifying the account types regardless of their activity level.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] BotHunter: An Approach to Detect Software Bots in GitHub - Ahmad Abdellatif, Mairieli Wessel, Igor Steinmacher, Marco Gerosa, Emad Shihab",""
"2022-05-18T02:29:00Z","2022-05-18T02:36:00Z","2024-03-21T14:46:32Z","","fc24c80e-9e2b-4710-b2db-06001de88e38@conf.researchr.org","","2022-05-03T15:38:48Z","\\textbf{Background:} Sub-optimal code is prevalent in software systems. Developers may write low-quality code due to many reasons, such as lack of technical knowledge, lack of experience, time pressure, management decisions, and even unhappiness. Once sub-optimal code is unknowingly (or knowingly) integrated into the codebase of software systems, its accumulation may lead to large maintenance costs and technical debt. Stack Overflow is a popular website for programmers to ask questions and share their code snippets. The crowdsourced and collaborative nature of Stack Overflow has created a large source of programming knowledge that can be leveraged to assist developers in their day-to-day activities. \n\\textbf{Objective:} In this paper, we present an exploratory study to evaluate the usefulness of recommending code improvements based on Stack Overflow answers’ edits. \n\\textbf{Method:} We propose Matcha, a code recommendation tool that leverages Stack Overflow code snippets with version history and code clone search techniques to identify sub-optimal code in software projects and suggest their optimised version. By using SOTorrent and GitHub datasets, we will quali-quantitatively investigate the usefulness of recommendations given by Matcha to developers using manual categorisation of the recommendations and acceptance of pull-requests to open-source projects.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Recommending Code Improvements Based on Stack Overflow Answer Edits - Chaiyong Ragkhitwetsagul, Matheus Paixao",""
"2022-05-18T02:36:00Z","2022-05-18T02:50:00Z","2024-03-21T14:46:32Z","","caecd4db-3b48-4f68-8f72-53e5bf9ecb76@conf.researchr.org","","2022-05-03T15:43:37Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-18T16:00:00Z","2022-05-18T16:04:00Z","2024-03-21T14:46:32Z","","b6e71de0-01b6-45a0-bd0e-2785c8904e97@conf.researchr.org","","2022-04-26T06:43:02Z","Understanding the practice of refactoring documentation is of paramount importance in academia and industry. Issue tracking systems are used by most software projects enabling developers, quality assurance, managers, and users to submit feature requests and other tasks such as bug fixing and code review. Although recent studies explored how to document refactoring in commit messages, little is known about how developers describe their refactoring needs in issues. In this study, we aim at exploring developer-reported refactoring changes in issues to better understand what developers consider to be problematic in their code and how they handle it. Our approach relies on text mining 45,477 refactoring-related issues and identifying refactoring patterns from a diverse corpus of 77 Java projects by investigating issues associated with 15,833 refactoring operations and developers’ explicit refactoring intention. Our results show that (1) developers mostly use move refactoring related terms/phrases to target refactoring related issues; and (2) developers tend to explicitly mention the improvement of specific quality attributes and focus on duplicate code removal. We envision our findings enabling tool builders to support developers with automated documentation of refactoring changes in issues.","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] An Exploratory Study on Refactoring Documentation in Issues Handling - Eman Abdullah AlOmar, Anthony Peruma, Mohamed Wiem Mkaouer, Christian D. Newman, Ali Ouni",""
"2022-05-18T16:04:00Z","2022-05-18T16:08:00Z","2024-03-21T14:46:32Z","","78b093cd-7242-413b-a9fb-39f31988afc4@conf.researchr.org","","2022-04-26T06:43:02Z","In recent years many Open-Source Software (OSS) projects have adopted various automations to automate repetitive tasks, one category of automations adopted by OSS are so-called bots. In previous work, researchers have found that the adoption of bots helps open-source developers merge more pull requests and reduces the need for communication between developers. The Apache Software Foundation (ASF) is a foundation that provides open-source software, and it supports the OSS projects that are a member of it. Projects that are a part of the ASF can choose to adopt the ASFBot, this bot automatically creates links between the JIRA issue tracker and pull requests (PRs) on GitHub. In this exploratory case study, we zoom in on the ASF ecosystem, and we seek to understand how the adoption of one specific bot (the ASFBot) impacts the discussions in the issue-trackers of these projects. In this study, we use the SmartShark dataset to investigate whether the ASFBot affects (i)human comments mentioning pull requests (PRs) and fixes in issue comments and (ii) the general human comment rate on issues. We apply a regression discontinuity design (RDD) on nine projects that are members of the ASF and which have been active both before and after the adoption of the ASFBot. Our results indicate (i) a decrease in comments mentioning pull requests and fixes after the bot adoption and (ii) no effect in the number of human comments after the bot adoption. By taking a first step towards understanding how the adoption of ASFBot impacts the issue tracker of projects we can better understand the advantages that the infrastructure of a foundation like ASF provides, and how it affects the commenting behavior of developers in the issue-tracker.","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Between JIRA and GitHub: ASFBot and its Influence on Human Comments in Issue Trackers - Ambarish Moharil, Dmitrii Orlov, Samar Jameel, Tristan Trouwen, Nathan Cassee, Alexander Serebrenik",""
"2022-05-18T16:08:00Z","2022-05-18T16:12:00Z","2024-03-21T14:46:32Z","","7c583802-6515-479a-ba72-324518aa6d2f@conf.researchr.org","","2022-04-26T06:43:02Z","Bug fixing and code refactoring are two distinct maintenance actions with different goals. While bug fixing is a corrective change that eliminates a defect from the program, refactoring targets improving the internal quality (i.e., maintainability) of a software system without changing its functionality. Best practices and common intuition suggest that these code actions should not be mixed in a single code change. Furthermore, as refactoring aims for improving quality without functional changes, we would expect that refactoring code changes will not be sources of bugs. Nonetheless, empirical studies show that none of the above hypotheses are necessarily true in practice. In this paper, we empirically investigate the interconnection between bug-related and refactoring code changes using the SmartSHARK dataset. Our goal is to explore how often bug fixes and refactorings co-occur in a single commit (tangled changes) and whether refactoring changes themselves might induce bugs into the system. We found that it is not uncommon to have tangled commits of bug fixes and refactorings; 21% of bug-fixing commits include at least one type of refactoring on average. What is even more shocking is that 54% of bug-inducing commits also contain code refactoring changes. For instance, 10% (652 occurrences) of the Change Variable Type refactorings in the dataset appear in bug-inducing commits that make up 7.9% of the total inducing commits.","2022-05-26T06:56:50Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Is Refactoring Always a Good Egg? Exploring the Interconnection Between Bugs and Refactorings - Amirreza Bagheri, Peter Hegedus",""
"2022-05-18T16:12:00Z","2022-05-18T16:16:00Z","2024-03-21T14:46:32Z","","dec4b8bf-9ebd-40ad-9838-0e35539e0184@conf.researchr.org","","2022-04-26T06:43:02Z","Refactoring is a widespread practice that aims to help improve the quality of a software system without altering its external behaviour. In practice, developers can perform refactoring operations on test and source code. However, while prior work shows that refactoring source code brings many benefits, a limited number of studies empirically investigate refactoring of test code and whether it is co-occurred with source code. To examine those co-occurring refactorings, we conducted an empirical study of 60,465 commits spanning 77 open-source Java projects. \nFirst, we quantitatively analyzed the commits from those projects to identify co-occurring refactoring commits (i.e., commits contain refactorings performed on test and source code). Our results showed that on average 17.9% of refactoring commits are co-occurring refactoring commits, which is twice as much as test code-only refactoring commits. Also, we investigated the type of refactorings applied to test code in those co-occurring commits. We found Change Variable Type and Move Class are the most common refactorings. Second, we trained random forest classifiers to predict when refactoring test code should co-occur with refactoring source code using features extracted from the refactoring source code in ten selected projects. Our results showed that the classifier can accurately predict when test and source code refactoring co-occurs with AUC values between 0.67-0.92. Our analysis also showed that the most important features for our classifier are related to the refactoring size and developer refactoring experience.","2022-05-31T23:13:08Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] On the Co-Occurrence of Refactoring of Test and Source Code - Nicholas Nagy, Rabe Abdalkareem",""
"2022-05-18T16:16:00Z","2022-05-18T16:20:00Z","2024-03-21T14:46:32Z","","d67b3d3e-433a-4922-bcf0-d8ccf54396bc@conf.researchr.org","","2022-04-26T06:43:02Z","To meet project timelines or budget constraints, developers intentionally deviate from writing optimal code to feasible code in what is known as incurring Technical Debt (TD). Furthermore, as part of planning their correction, developers document these deficiencies as comments in the code (i.e., self-admitted technical debt or SATD). As a means of improving source code quality, developers often apply a series of refactoring operations to their codebase. In this study, we explore developers repaying this debt through refactoring operations by examining occurrences of SATD removal in the code of 76 open-source Java systems. Our findings show that TD payment usually occurs with refactoring activities and developers refactor their code to remove TD for specific reasons. We envision our findings supporting vendors in providing tools to better support developers in the automatic repayment of technical debt.","2022-05-14T14:17:12Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Refactoring Debt: Myth or Reality? An Exploratory Study on the Relationship Between Technical Debt and Refactoring - Anthony Peruma, Eman Abdullah AlOmar, Christian D. Newman, Mohamed Wiem Mkaouer, Ali Ouni",""
"2022-05-18T16:20:00Z","2022-05-18T16:24:00Z","2024-03-21T14:46:32Z","","2d7de479-e387-4054-951d-9624052e9304@conf.researchr.org","","2022-04-26T06:43:02Z","Buggy software impacts people’s lives and businesses. Nowadays, a huge portion of a software project’s cost is spent on debugging (finding and fixing bugs). Therefore, reducing the time needed to release new software versions free from bugs becomes crucial. Continuous delivery (CD) arises as an alternative to traditional software release engineering by providing the capability to faster and continuously release software to customers through automated pipelines. Previous studies claim that CD adoption leads to a reduction in the software release cycle time, including the time lag to fix reported bugs (bug-fixing time) and apply correction patches in the affected versions. However, there is a lack of empirical evidence supporting (or not) this claim. To fulfill this gap, we conducted an empirical study to evaluate the impact of CD adoption in the bug-fixing time. We study 25 open-source projects comparing the bug-fixing time before and after adopting CD. Our results show that bug-fixing time after CD adoption becomes shorter (with statistical significance) than the bug-fixing time before CD adoption.","2022-05-13T05:04:45Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Studying the Impact of Continuous Delivery Adoption on Bug-Fixing Time in Apache’s Open-Source Projects - Carlos Diego Andrade de Almeida, Diego N. Feijó, Lincoln Souza Rocha",""
"2022-05-18T16:24:00Z","2022-05-18T16:28:00Z","2024-03-21T14:46:32Z","","4517ff78-13cb-4d88-9bc4-ce639b4db03e@conf.researchr.org","","2022-04-26T06:43:02Z","In pull-based development systems, code reviews and pull request comments play important roles in improving code quality. In such systems, reviewers attempt to carefully check a piece of code by different unit tests. Unfortunately, sometimes they miss bugs in their review of pull requests, which lead to quality degradations of the systems. In other words, disastrous consequences occur when bugs are observed after merging the pull requests. The lack of a concrete understanding of these bugs led us to investigate them and categorize them. In this research, we try to identify missed bugs in pull requests of SmartSHARK dataset projects. Our contribution is twofold. First, we hypothesized merged pull requests that have code reviews, code review comments, or pull request comments after merging, may have missed bugs after the code review. We considered these merged pull requests as candidate pull requests having missed bugs. Based on our assumption, we obtained 3,261 candidate pull requests from 77 open-source GitHub projects. After two rounds of restrictive manual analysis, we found 187 bugs missed in 173 pull requests. In the first step, we found 224 buggy pull requests containing missed bugs after merging the pull requests. Secondly, we defined and finalized a taxonomy that is appropriate for the bugs that we found and then found the distribution of bug categories after analyzing those pull requests all over again. The categories of missed bugs in pull requests and their distributions are: semantic (51.34%), build (15.5%), analysis checks (9.09%), compatibility (7.49%), concurrency (4.28%), configuration (4.28%), GUI (2.14%), API (2.14%), security (2.14%), and memory (1.6%).","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Which bugs are missed in code reviews: An empirical study on SmartSHARK dataset - fatemeh khoshnoud, Ali Rezaei Nasab, Zahra Toudeji, Ashkan Sami",""
"2022-05-18T16:28:00Z","2022-05-18T16:50:00Z","2024-03-21T14:46:32Z","","ca77ba1c-2add-4076-8ec9-21341c05bb16@conf.researchr.org","","2022-05-03T15:48:57Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-18T18:00:00Z","2022-05-18T18:07:00Z","2024-03-21T14:46:32Z","","fa985d56-2a27-4beb-9194-c700de84eb50@conf.researchr.org","","2022-04-26T06:45:54Z","The automotive industry has transitioned from being electro-mechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook’s front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing a first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing their characteristics and development styles. \nThe landscape is defined by 15,000+ users contributing to ~600 actively-developed automotive projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe disruptions along multiple dimensions, including shift in preferred language from MATLAB to Python and prevalence of perception and decision related software over traditional automotive software. This study witnesses open source automotive software boom in its infancy with huge potential and implications for future research and practice.","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Painting the Landscape of Automotive Software in GitHub - Sangeeth Kochanthara, Yanja Dajsuren, Loek Cleophas, Mark van den Brand",""
"2022-05-18T18:07:00Z","2022-05-18T18:14:00Z","2024-03-21T14:46:32Z","","49f2e4b9-1c6e-409d-af92-b0e7abd95ce4@conf.researchr.org","","2022-04-26T06:45:54Z","Conventionally, callbacks and inversion of control have been the main tools to structure event-driven applications. Sadly, those patterns constitute a well-known source of design problems. The Reactive Programming (RP) paradigm has arisen as an approach to mitigate these problems. Yet, little evidence has been provided regarding the advantages of RP, and concerns have also arisen about the API usability of RP libraries given their disparate number of operators. In this work, we conduct a study on GitHub (GH) and Stack Overflow (SO) and explore three Reactive Extensions (Rx) libraries (RxJava, RxJS, and RxSwift) with the most GH projects to understand how much the vast Rx operators are being used. Also, we examine Rx SO posts to complement the results from the GH exploration by understanding the problems faced by RP developers and how they relate with the operators’ frequencies found in open source projects. Results reveal that, in spite of its API size, the great majority of the Rx operators are actually being used (95.2%), with only a few, mostly related to RxJava, not being utilized. Also, we unveil 23 topics from SO with more posts concerning the Stream Abstraction (36.4%). Posts related to Dependency Management, Introductory Questions, and iOS Development figure as relevant topics to the community. The findings herein present can not only stimulate advancements in the field by understanding the usage of RP API and the main problems faced by developers, but also help newcomers in identifying the most important operators and the areas that are the most likely to be relevant for a RP application.","2022-05-16T23:01:16Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Mining the Usage of Reactive Programming APIs: A Study on GitHub and Stack Overflow - Carlos Zimmerle, Kiev Gama, Fernando Castor, José Murilo Filho",""
"2022-05-18T18:14:00Z","2022-05-18T18:18:00Z","2024-03-21T14:46:32Z","","7c699eed-bd5f-4463-bb76-4b70a31fc2f3@conf.researchr.org","","2022-04-26T06:45:54Z","Numerous tools exist for mining source code and software development process metrics. However, very few publicly available tools focus on source code comments, a crucial software artifact. This paper presents SoCCMiner (Source Code-Comments and Comment-Context Miner), a tool that offers multiple mining pipelines. It is the first readily available (plug-and-play) and customizable open-source tool for mining source code contextual information of comments at different granularities (Class comments, Method comments, Interface comments, and other granular comments). Mining comments at different source code granularities can aid researchers and practitioners working in a host of applications that focus on source code comments, such as Self-Admitted Technical Debt, Program Comprehension, and other applications. Furthermore, it is highly adaptable and extendable to include additional attributes and support other programming languages. This prototype supports the Java programming language.","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] SoCCMiner: A Source Code-Comments and Comment-Context Miner - Murali Sridharan, Mika Mäntylä, Maëlick Claes, Leevi Rantala",""
"2022-05-18T18:18:00Z","2022-05-18T18:22:00Z","2024-03-21T14:46:32Z","","8e163b49-6581-4159-8dd5-ff1dd23fdd14@conf.researchr.org","","2022-04-26T06:45:54Z","MATLAB/Simulink is widely used for model-based design. Engineers create Simulink models and compile them to embedded code, often to control safety-critical cyber-physical systems in automotive, aerospace, and healthcare applications. Despite Simulink’s importance, there are few large-scale empirical Simulink studies, perhaps because there is no large readily available corpus of third-party open-source Simulink models. To enable empirical Simulink studies, this paper introduces SLNET, the largest corpus of freely available third-party Simulink models. SLNET has several advantages over earlier collections. Specifically, SLNET is 8 times larger than the largest previous corpus of Simulink models, includes fine-grained metadata, is constructed automatically, is self-contained, and allows redistribution. SLNET is available under permissive open-source licenses and contains all of its collection and analysis tools.","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] SLNET: A Redistributable Corpus of 3rd-party Simulink Models - Sohil Lal Shrestha, Shafiul Azam Chowdhury, Christoph Csallner",""
"2022-05-18T18:22:00Z","2022-05-18T18:26:00Z","2024-03-21T14:46:32Z","","106b8179-0f1c-4ccd-b48b-af0c34c2022b@conf.researchr.org","","2022-04-26T06:45:54Z","Stack Overflow (SO) is becoming an indispensable part of the modern software development workflow. However, navigating SO posts and comparing different solutions is time-consuming and cumbersome given the limited time, attention, and memory capacity of programmers. Recent research has proposed to summarize SO posts to concise text to help programmers quickly decide the relevance and quality of SO posts. Yet there is no large, comprehensive dataset of high-quality SO post summaries, which hinders the development and evaluation of post summarization techniques. We present SOSum, a dataset of 2278 popular SO posts with manually labeled summative sentences. Questions in SOSum cover 669 tags with a median view count of 253K and a median post score of 17. This dataset will foster research on sentence-level summarization of SO posts and has the potential to facilitate text summarization research on other types of textual software artifacts such as programming tutorials.","2022-05-14T09:36:49Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] SOSum: A Dataset of Stack Overflow Post Summaries - Bonan Kou, Yifeng Di, Muhao Chen, Tianyi Zhang",""
"2022-05-18T18:26:00Z","2022-05-18T18:30:00Z","2024-03-21T14:46:32Z","","2b661f0b-0ed2-4461-9f13-48b3a060abba@conf.researchr.org","","2022-04-26T06:45:54Z","This works presents inspect4py, a static code analysis framework designed to automatically extract the main features, metadata and documentation of Python code repositories. Given an input folder with code, inspect4py uses abstract syntax trees and state of the art tools to find all functions, classes, tests, documentation, call graphs, module dependencies and control flows within all code files in that repository. Using these findings, inspect4py infers different ways of invoking a software component. We have evaluated our framework on 95 annotated repositories, obtaining promising results for software type classification (over 95% F1-score). With inspect4py, we aim to ease the understandability and adoption of software repositories by other researchers and developers.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Inspect4py: A Knowledge Extraction Framework for Python Code Repositories - Rosa Filgueira, Daniel Garijo",""
"2022-05-18T18:30:00Z","2022-05-18T18:34:00Z","2024-03-21T14:46:32Z","","3e71b746-453a-4c78-a450-8e222bae2890@conf.researchr.org","","2022-04-26T06:45:54Z","Today, software developers work on complex and fast-moving projects that often require instant assistance from other domain and subject matter experts. Chat servers such as Discord facilitate live communication and collaboration among developers all over the world. With numerous topics discussed in parallel, mining and analyzing the chat data of these platforms would offer researchers and tool makers opportunities to develop software tools and services such as automated virtual assistants, chatbots, chat summarization techniques, Q&amp;A thesaurus, and more. \nIn this paper, we propose a dataset called DISCO consisting of the one-year public DIScord chat COnversations of four software development communities (Python, Go, Clojure, Racket). We have collected the chat data of the channels containing general programming Q&amp;A discussions from the four Discord servers, applied a disentanglement technique to extract conversations from the chat transcripts, and performed a manual validation of conversations on a random sample (500 conversations). Our dataset consists of 28,712 conversations, 1,508,093 messages posted by 323,562 users. As a case study on the dataset, we applied a topic modeling technique for extracting the top five general topics that are most discussed in each Discord channel.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] DISCO: A Dataset of Discord Chat Conversations for Software Engineering Research - Keerthana Muthu Subash, Lakshmi Prasanna Kumar, Sri Lakshmi Vadlamani, Preetha Chatterjee, Olga Baysal",""
"2022-05-18T18:34:00Z","2022-05-18T18:50:00Z","2024-03-21T14:46:32Z","","58fbaef3-75ec-4eec-bf9a-5f684531ed92@conf.researchr.org","","2022-05-04T03:44:47Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-19T00:00:00Z","2022-05-19T00:04:00Z","2024-03-21T14:46:32Z","","c4a3074e-cbef-48c7-940c-5e0910975c2d@conf.researchr.org","","2022-04-26T06:47:00Z","To assess the quality of a test suite, one can rely on mutation testing, which computes whether the overall test cases are adequately exercising the covered lines. However, this high level of granularity may overshadow the quality of individual test methods. Thus, we propose an empirical study of high-quality test methods by mutation testing. We analyze over 18K test methods from popular software projects and show empirical evidence that high-quality test methods: (1) are slightly smaller; (2) have fewer modifications over time; (3) are less affected by critical test smells. Lastly, we present practical implications for researchers and practitioners.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Characterizing High-Quality Test Methods: A First Empirical Study - Victor Veloso, Andre Hora",""
"2022-05-19T00:04:00Z","2022-05-19T00:11:00Z","2024-03-21T14:46:32Z","","ad06890f-7662-49c1-9ffd-f01ecc46e274@conf.researchr.org","","2022-04-26T06:47:00Z","Gameplay videos contain rich information about how players interact with the game and how the game responds. Sharing gameplay videos on social media platforms, such as Reddit, has become a common practice for many players. Often, players will share gameplay videos that showcase video game bugs. Such gameplay videos are software artifacts that can be utilized for game testing, as they provide insight for bug analysis. Although large repositories of gameplay videos exist, parsing and mining them in an effective and structured fashion has still remained a big challenge. In this paper, we propose a search method that accepts any English text query as input to retrieve relevant videos from large repositories of gameplay videos. Our approach does not rely on any external information (such as video metadata); it works solely based on the content of the video. By leveraging the zero-shot transfer capabilities of the Contrastive Language-Image Pre-Training (CLIP) model, our approach does not require any data labeling or training. To evaluate our approach, we present the GamePhysics dataset consisting of 26,954 videos from 1,873 games, that were collected from the GamePhysics section on the Reddit website. Our approach shows promising results in our extensive analysis of simple queries, compound queries, and bug queries, indicating that our approach is useful for object and event detection in gameplay videos. An example application of our approach is as a gameplay video search engine to aid in reproducing video game bugs. A demo of our approach can be found at the following link: HTTP://165.232.141.160:50001.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning - Mohammad Reza Taesiri, Finlay Macklon, Cor-Paul Bezemer",""
"2022-05-19T00:11:00Z","2022-05-19T00:18:00Z","2024-03-21T14:46:32Z","","2b1bd126-0808-4216-88f9-44c8ec57daee@conf.researchr.org","","2022-04-26T06:47:00Z","Code metrics have been widely used to estimate software maintenance effort. Metrics have generally been used to guide developer effort to reduce or avoid future maintenance burdens. Size is the simplest and most widely deployed metric. The size metric is pervasive because size correlates with many other common metrics (e.g., McCabe complexity, readability, etc.). Given the ease of computing a method’s size, and the ubiquity of these metrics in industrial settings, it is surprising that no systematic study has been performed to provide developers with meaningful method size guidelines with respect to future maintenance effort. In this paper we examine the evolution of ∼785K Java methods and show that developers should strive to keep their Java methods under 24 lines in length. Additionally, we show that decomposing larger methods to smaller methods also decreases overall maintenance efforts. Taken together, these findings provide empirical guidelines to help developers design their systems in a way that can reduce future maintenance.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] An Empirical Study on Maintainable Method Size in Java - Shaiful Chowdhury, Gias Uddin, Reid Holmes",""
"2022-05-19T00:18:00Z","2022-05-19T00:25:00Z","2024-03-21T14:46:32Z","","7db7f84c-e0e0-4057-b995-e10e92e590cd@conf.researchr.org","","2022-04-26T06:47:00Z","While Python is increasingly popular, program analysis tooling for Python is lagging. This is due, in part, to complex features of the Python language. In addition to the ``usual suspects'', reflection and dynamic execution, the Python language introduces other complex features with difficult to understand and model semantics, e.g., context managers, decorators, and generators, among others. This paper explores how often and in what ways developers use certain complex features. We address three research questions: (i)~How often do developers use certain complex Python features? (ii)~In what ways to developers use these features? (iii)~Does use of complex features increase or decrease over time? \nOur findings show that usage of dynamic features that pose a threat to static analysis is infrequent. On the other hand, usage of context managers and decorators is surprisingly widespread. Our actionable result is a list of Python features that any ``minimal syntax'' ought to handle in order to capture developers’ use of the Python language. Understanding the usage of Python features can help guide tool-builders and researchers towards building sound and efficient program analysis tools for Python.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Complex Python Features in the Wild - Yi Yang, Ana Milanova, Martin Hirzel",""
"2022-05-19T00:25:00Z","2022-05-19T00:29:00Z","2024-03-21T14:46:32Z","","4e51e855-b2c3-4e4c-893b-15591aac3246@conf.researchr.org","","2022-04-26T06:47:00Z","Unit testing is an essential part of the software development process, which helps to identify issues with source code in early stages of development and prevent regressions. Machine learning has emerged as viable approach to help software developers generate automated unit tests. However, generating reliable unit test cases that are semantically correct and capable of catching software bugs or unintended behavior via machine learning requires large, metadata-rich, datasets. In this paper we present Methods2Test: a large, supervised dataset of test cases mapped to corresponding methods under test (i.e., focal methods). This dataset contains 780,944 pairs of JUnit tests and focal methods, extracted from a total of 91,385 Java open source projects hosted on GitHub with licenses permitting re-distribution. The main challenge behind the creation of the Methods2Test was to establish a reliable mapping between a test case and the relevant focal method. To this aim, we designed a set of heuristics, based on developers’ best practices in software testing, which identify the likely focal method for a given test case. To facilitate further analysis, we store a rich set of metadata for each method-test pair in JSON-formatted files. Additionally, we extract textual corpus from the dataset at different context levels, which we provide both in raw and tokenized forms, in order to enable researchers to train and evaluate machine learning models for Automated Test Generation. Methods2Test is publicly available at: https://github.com/microsoft/methods2test","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Methods2Test: A dataset of focal methods mapped to test cases - Michele Tufano, Shao Kun Deng, Neel Sundaresan, Alexey Svyatkovskiy",""
"2022-05-19T00:29:00Z","2022-05-19T00:33:00Z","2024-03-21T14:46:32Z","","9c512dd8-6d01-4989-b38a-464c7c4dbcc9@conf.researchr.org","","2022-04-26T06:47:00Z","The static properties of code repositories, e.g., lines of code, dependents, dependencies, etc. can be readily scraped from code hosting platforms such as GitHub, and from package management systems such as npm for JavaScript; Although no less important, information related to the \\textit{dynamic} properties of programs, e.g., number of tests in a test suite that pass or fail, is less readily available. The ability to easily collect this dynamic information could be immensely useful to researchers conducting corpus analyses, as they could differentiate projects based on properties that can only be observed by running them. \nIn this paper, we present npm-filter, an automated tool that can download, install, build, test, and run custom user scripts over the source code of JavaScript projects available on npm, the most popular JavaScript package manager. We outline this tool, describe its implementation, and show that npm-filter has already been useful in developing evaluation suites for multiple JavaScript tools.","2022-05-14T09:43:28Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] npm-filter: Automating the mining of dynamic information from npm packages - Ellen Arteca, Alexi Turcotte",""
"2022-05-19T00:33:00Z","2022-05-19T00:37:00Z","2024-03-21T14:46:32Z","","06e05923-56b0-4e8e-bce9-6c44a684de71@conf.researchr.org","","2022-04-26T06:47:00Z","In this paper, we present ManyTypes4TypeScript, a very large corpus for training and evaluating machine-learning models for sequence-based type inference in TypeScript. The dataset includes over 9 million type annotations, across 13,953 projects and 539,571 files. The dataset is approximately 10x larger than analogous type inference datasets for Python, and is the largest available for TypeScript. We also provide API access to the dataset, which can be integrated into any tokenizer and used with any state-of-the-art sequence-based model. Finally, we provide analysis and performance results for state-of-the-art code-specific models, for baselining. ManyTypes4TypeScript is available on Huggingface, Zenodo, and CodeXGLUE.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] ManyTypes4TypeScript: A Comprehensive TypeScript Dataset for Sequence-Based Type Inference - Kevin Jesse, Prem Devanbu",""
"2022-05-19T00:37:00Z","2022-05-19T00:50:00Z","2024-03-21T14:46:32Z","","79b13c2d-eb13-4956-8a69-7c8be7a59d85@conf.researchr.org","","2022-05-04T03:47:23Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-18T07:00:00Z","2022-05-18T07:04:00Z","2024-03-21T14:46:32Z","","b245210e-398c-42cd-b5cd-0bbab1572510@conf.researchr.org","","2022-04-29T07:31:51Z","Organisations use issue tracking systems (ITSs) to track and document their projects’ work in units called issues. This style of documentation encourages evolutionary refinement, as each issue can be independently improved, commented on, linked to other issues, and progressed through the organisational workflow. Commonly studied ITSs so far include GitHub, GitLab, and Bugzilla, while Jira, one of the most popular ITS in practice with a wealth of additional information, has yet to receive such attention. Unfortunately, diverse public Jira datasets are rare, likely due to the difficulty in finding and accessing these repositories. With this paper, we release a dataset of 16 public Jiras with 1822 projects, spanning 2.7 million issues with a combined total of 32 million changes, 9 million comments, and 1 million issue links. We believe this Jira dataset will lead to many fruitful research projects investigating issue evolution, issue linking, cross-project analysis, as well as cross-tool analysis when combined with existing well-studied ITS datasets.","2022-05-04T16:44:35Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] An Alternative Issue Tracking Dataset of Public Jira Repositories - Lloyd Montgomery, Clara Marie Lüders, Walid Maalej",""
"2022-05-18T07:04:00Z","2022-05-18T07:11:00Z","2024-03-21T14:46:32Z","","323974f5-e8d4-49f4-84e7-51cbd4fd0a07@conf.researchr.org","","2022-04-29T07:31:51Z","Infrastructure as Code is the practice of automating the provisioning, configuration, and orchestration of network nodes using code in which variable values such as configuration parameters, node hostnames, etc. play a central role. Mistakes in these values are an important cause of infrastructure defects and corresponding outages. Ansible, a popular IaC language, nonetheless features semantics which can cause confusion about the value of variables. \nIn this paper, we identify six novel code smells related to Ansible’s intricate variable precedence rules and lazy-evaluated template expressions. Their detection requires an accurate representation of control and data flow, for which we transpose the program dependence graph to Ansible. We use the resulting detector to empirically investigate the prevalence of these variable smells in 21,931 open-source Ansible roles, uncovering 31,334 unique smell instances across 4,260 roles. We observe an upward trend in the number of variable smells over time, that it may take a long time before they are fixed, and that code changes more often introduce new smells than fix existing ones. Our results are a call to arms for more in-depth quality checkers for IaC code, and highlight the importance of transcending syntax in IaC research.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Smelly Variables in Ansible Infrastructure Code: Detection, Prevalence, and Lifetime - Ruben Opdebeeck, Ahmed Zerouali, Coen De Roover",""
"2022-05-18T07:11:00Z","2022-05-18T07:18:00Z","2024-03-21T14:46:32Z","","a05b52fe-b7f2-465b-95d6-816bf20e3c33@conf.researchr.org","","2022-04-29T07:33:15Z","Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, and Subtask. While previous research has focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. \nFor this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal / Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, as expected, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal / Causal links. \nMotivated by the differences between the types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on our JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Beyond Duplicates: Towards Understanding and Predicting Link Types in Issue Tracking Systems - Clara Marie Lüders, Abir Bouraffa, Walid Maalej",""
"2022-05-18T07:18:00Z","2022-05-18T07:25:00Z","2024-03-21T14:46:32Z","","e24e4c71-ad2b-4cfd-af76-eede2e2c23af@conf.researchr.org","","2022-04-29T08:01:57Z","In the early stage of development, developers copied internal code snippets to enhance developing efficiency. With the rapid growth of Bytedance, similar or equivalent code snippets across different repositories or different product lines’ codebases would potentially increase the cost of maintenance and distribute vulnerable code snippets. With the application of Clone-Detection, there are multi- ple well-established tools or techniques used for detecting similar code snippets in Java, JavaScript, Objective C and etc. We could hardly find similar tools available for Go, a widely-used program- ming language in the field of server development, especially at Bytedance. For the lack of public and labeled datasets, we utilized a great number of code snippets in Bytedance’s codebase and trained an unsupervised model to propose GoCopyCatch (GoCC), a tool and technique for Clone-Detection in Go.","2022-05-04T03:54:56Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Real-World Clone-Detection in Go - Qinyun Wu, Huan Song, Ping Yang",""
"2022-05-18T07:25:00Z","2022-05-18T07:29:00Z","2024-03-21T14:46:32Z","","94b0535e-5201-4392-bef5-7290cf3fe73f@conf.researchr.org","","2022-04-29T08:02:29Z","\\textit{Context.} The game industry is increasingly growing in recent years. Every day, millions of people play video games, not only as a hobby, but also for professional competitions (e.g., e-sports or speedrunning) or for making business by entertaining others (e.g., streamers). The latter daily produce a large amount of gameplay videos in which they also comment live what they experience. Since no software and, thus, no video game is perfect, streamers may encounter several problems (such as bugs, glitches, or performance issues). However, it is unlikely that they explicitly report such issues to developers. The identified problems may negatively impact the user’s gaming experience and, in turn, can harm the reputation of the game and of the producer. \\textit{Objective.} We aim at proposing and empirically evaluating GELID, an approach for automatically extracting relevant information from gameplay videos by (i) identifying video segments in which streamers experienced anomalies; (ii) categorizing them based on their type and context in which appear (e.g., bugs or glitches appearing in a specific level or scene of the game); and (iii) clustering segments that regard the same specific issue. \\textit{Method.} We will build on top of existing approaches able to identify videos that are relevant for a specific video game. These represent the input of GELID that processes them to achieve the defined objectives. We will experiment GELID on several gameplay videos to understand the extent to which each of its steps is effective.","2022-05-04T03:55:07Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Towards Using Gameplay Videos for Detecting Issues in Video Games - Emanuela Guglielmi, Simone Scalabrino, Gabriele Bavota, Rocco Oliveto",""
"2022-05-18T07:29:00Z","2022-05-18T07:33:00Z","2024-03-21T14:46:32Z","","7353f98c-5a62-4006-80f9-bfab609d164a@conf.researchr.org","","2022-04-29T07:31:51Z","Background. From information theory, surprisal is a measurement of how unexpected a particular event is. Statistical language models provide a probabilistic approximation of natural languages, and because surprisal is constructed with the probability of an event occuring, it is therefore possible to determine the surprisal associated with English sentences. The issues and pull requests of software repository issue trackers provide insight into the development process and likely contain the surprising events of this process. \nObjective. Prior works have identified that unusual events in software repositories are of interest to developers, and use simple code metrics-based methods for detecting them. In this study we will propose a new method for unusual event detection in software repositories using surprisal. With the ability to find surprising issues and pull requests, we intend to further analyse them to determine if they actually hold importance in a repository, or if they pose a significant challenge to address. If it is possible to find bad surprises early, or before they cause additional troubles, it is plausible that effort, cost and time will be saved as a result. \nMethod. After extracting the issues and pull requests from 5000 of the most popular software repositories on GitHub, we will train a language model to represent these issues. We will then measure their perceived importance in the repository, measure their resolution difficulty using several analogues, measure the surprisal of each, and finally generate inferential statistics to describe any correlations.","2022-05-05T17:06:18Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Is Surprisal in Issue Trackers Actionable? - James Caddy, Markus Wagner , Christoph Treude, Earl T. Barr, Miltiadis Allamanis",""
"2022-05-18T07:33:00Z","2022-05-18T07:50:00Z","2024-03-21T14:46:32Z","","b02ac612-893b-4990-9384-89911079595a@conf.researchr.org","","2022-05-03T15:45:33Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-18T09:00:00Z","2022-05-18T09:04:00Z","2024-03-21T14:46:32Z","","16d22104-05f5-4bdf-b3c8-bbd80500790b@conf.researchr.org","","2022-04-26T06:41:29Z","We conduct an exploratory, large-scale, longitudinal study of 50 years of commits to publicly available version control system repositories, in order to characterize the geographic diversity of contributors to public code and its evolution over time. We analyze in total 2.2 billion commits collected by Software Heritage from 160 million projects and authored by 43 million authors during the 1971-2021 time period. We geolocate developers to 12 world regions derived from the United Nation geoscheme, using as signals email top-level domains, author names compared with names distributions around the world, and UTC offsets mined from commit metadata. We find evidence of the early dominance of North America in open source software, later joined by Europe. After that period, the geographic diversity in public code has been constantly increasing. We also identify relevant historical shifts related to the UNIX wars, the increase of coding literacy in Central and South Asia, and broader phenomena like colonialism and people movement across countries (immigration/emigration).","2022-05-14T09:47:39Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Geographic Diversity in Public Code Contributions - Davide Rossi, Stefano Zacchiroli",""
"2022-05-18T09:04:00Z","2022-05-18T09:11:00Z","2024-03-21T14:46:32Z","","8aff7597-1dc9-42a4-9f9e-ea5c419cd06f@conf.researchr.org","","2022-04-26T06:41:29Z","Quantitative studies on the border between Mining Software Repository (MSR) and Empirical Software Engineering (ESE) apply data analysis methods, like regression modeling, statistic tests or correlation analysis, to commits or pulls to better understand the software development process. Such studies assure the validity of the reported results by following a sound methodology. However, with increasing complexity, parts of the methodology can still go wrong. This may result in MSR/ESE studies with undetected threats to validity. In this paper, we propose to systematically protect against threats by operationalizing their treatment using simulations. A simulation substitutes observed and unobserved data, related to an MSR/ESE scenario, with synthetic data, carefully defined according to plausible assumptions on the scenario. Within a simulation, unobserved data becomes transparent, which is the key difference to a real study, necessary to detect threats to an analysis methodology. Running an analysis methodology on synthetic data may detect basic technical bugs and misinterpretations, but it also improves the trust in the methodology. The contribution of a simulation is to operationalize testing the impact of important assumptions. Assumptions still need to be rated for plausibility. We evaluate simulation-based testing by operationalizing undetected threats in the context of four published MSR/ESE studies. We recommend that future research uses such more systematic treatment of threats, as a contribution against the reproducibility crisis.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Operationalizing Threats to MSR Studies by Simulation-Based Testing - Johannes Härtel, Ralf Laemmel",""
"2022-05-18T09:11:00Z","2022-05-18T09:15:00Z","2024-03-21T14:46:32Z","","578492d4-2b8e-497e-a1cb-5c38ec442fb4@conf.researchr.org","","2022-04-26T06:41:29Z","We introduce the General Index of Software Engineering Papers, a dataset of fulltext-indexed papers from the most prominent scientific venues in the field of Software Engineering. The dataset includes both complete bibliographic information and indexed n-grams (sequence of contiguous words after removal of stopwords and non-words, for a total of 577’276’382 unique n-grams in this release) with length 1 to 5 for 44’581 papers retrieved from 34 venues over the 1971–2020 period. The dataset serves use cases in the field of meta-research, allowing to introspect the output of software engineering research even when access to papers or scholarly search engines is not possible (e.g., due to contractual reasons). The dataset also contributes to making such analyses reproducible and independently verifiable, as opposed to what happens when they are conducted using 3rd-party and non-open scholarly indexing services. The dataset is available as a portable Postgres database dump and released as open data.","2022-05-14T09:47:39Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] The General Index of Software Engineering Papers - Zeinab Abou Khalil, Stefano Zacchiroli",""
"2022-05-18T09:15:00Z","2022-05-18T09:22:00Z","2024-03-21T14:46:32Z","","d5719bf4-6dae-414b-a0c0-baac896b9ada@conf.researchr.org","","2022-04-26T06:41:29Z","Microtask programming is a solution to promote distributed development such as remote work in industry. The key idea of microtask programming in distributed development is to reduce face-to-face communication across developers by splitting the development task of software into independent microtasks. Our research team reported that microtask programming has potential benefits such as the fluidity of project assignments in industrial companies. However, we suppose it still has challenges. We found three key challenges that lie ahead to employ microtask programming in industry by our interview: well-being, motivation, and responsibility. Our presentation will describe our interview, these challenges, and future research direction.","2022-05-16T12:52:40Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Challenges and Future Research Direction for Microtask Programming in Industry - Masanari Kondo, Shinobu Saito, IIMURA Yukako, Eunjong Choi, Osamu Mizuno, Yasutaka Kamei, Naoyasu Ubayashi",""
"2022-05-18T09:22:00Z","2022-05-18T09:29:00Z","2024-03-21T14:46:32Z","","6d44318b-25aa-42ab-8d80-b4ff28df7ccb@conf.researchr.org","","2022-04-26T06:41:29Z","InnerSource is the application of best open source practices within the walls of the organization. Large corporations are required to be more and more efficient in the development of software and even more in the banking industry. There are three main areas of expenditure: infrastructure and facilities, people, and technology. The latter is of importance nowadays as key for the business and core to this paper. Reusability and collaboration are some of the ways a large corporation can be more efficient in technology. By being able to discover existing software and collaborating across business units, departments, or even geographical regions, corporations can share effort across them, and avoid starting once and again a similar piece of software.","2022-05-18T10:22:20Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Starting the InnerSource Journey: Key Goals and Metrics to Measure Collaboration - Daniel Izquierdo-Cortazar, Jesús Alonso-Gutiérrez, Alberto Pérez García-Plaza, Gregorio Robles, Jesus M. Gonzalez-Barahona",""
"2022-05-18T09:29:00Z","2022-05-18T09:33:00Z","2024-03-21T14:46:32Z","","b8ce8456-ed0f-41eb-92c7-36f927a87852@conf.researchr.org","","2022-04-26T06:41:29Z","Context: Forgetting is defined as a gradual process of losing information. Even though there are many studies demonstrating the effect of forgetting in software development, to the best of our knowledge, no study explores the impact of forgetting in software development using a controlled experiment approach. Objective: We would like to provide insights on the impact of forgetting in software development projects. We want to examine whether the recency &amp; frequency of interaction impact forgetting in software development. Methods: We will conduct an experiment that examines the impact of forgetting in software development. Participants will first do an initial task. According to their initial task performance, they will be assigned to either the experiment or the control group. The experiment group will then do two additional tasks to enhance their exposure to the code. Both groups will then do a final task to see if additional exposure to the code benefits the experiment group’s performance in the final task. Finally, we will conduct a survey and a recall task with the same participants to collect data about their perceptions of forgetting and quantify their memory performance, respectively","2022-05-14T09:47:39Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Investigating the Impact of Forgetting in Software Development - Utku Unal, Eray Tüzün, Tamer Gezici, Ausaf Ahmed Farooqui",""
"2022-05-18T09:33:00Z","2022-05-18T09:50:00Z","2024-03-21T14:46:32Z","","72f25023-6863-45ad-81a1-7354cbfbff9c@conf.researchr.org","","2022-05-03T15:46:26Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-18T17:00:00Z","2022-05-18T17:07:09Z","2024-03-21T14:46:32Z","","5b490d83-6c56-4eb5-b116-93ab44f37692@conf.researchr.org","","2022-04-26T06:44:26Z","Background: Machine learning techniques have been widely used and demonstrate promising performance in many software security tasks such as software vulnerability prediction. However, the class ratio within software vulnerability datasets is often highly imbalanced (since the percentage of observed vulnerability is usually very low). Goal: To help security practitioners address software security data class imbalanced issues and further help build better prediction models with resampled datasets. Method: We introduce an approach called Dazzle which is an optimized version of conditional Wasserstein Generative Adversarial Networks with gradient penalty (cWGAN-GP). Dazzle explores the architecture hyperparameters of cWGAN-GP with a novel optimizer called Bayesian Optimization. We use Dazzle to generate minority class samples to resample the original imbalanced training dataset. Results: We evaluate Dazzle with three software security datasets, i.e., Moodle vulnerable files, Ambari bug reports, and JavaScript function code. We show that Dazzle is practical to use and demonstrates promising improvement over existing state-of-the-art oversampling techniques such as SMOTE (e.g., with an average of about 60% improvement rate over SMOTE in recall among all datasets). Conclusion: Based on this study, we would suggest the use of optimized GANs as an alternative method for security vulnerability data class imbalanced issues.","2022-05-04T03:25:42Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Dazzle: Using Optimized Generative Adversarial Networks to Address Security Data Class Imbalance Issue - Rui Shu, Tianpei Xia, Laurie Williams, Tim Menzies",""
"2022-05-18T17:07:09Z","2022-05-18T17:14:17Z","2024-03-21T14:46:32Z","","d8656528-feea-4b7b-a97e-dbd9a0ea93e7@conf.researchr.org","","2022-04-26T06:44:26Z","Deep Learning (DL) models have been widely used to support code completion. These models, once properly trained, can take as input an incomplete code component (e.g., an incomplete function) and predict the missing tokens to finalize it. GitHub Copilot is an example of code recommender built by training a DL model on millions of open source repositories: The source code of these repositories acts as training data, allowing the model to learn “how to program”. The usage of such a code is usually regulated by Free and Open Source Software (FOSS) licenses, that establish under which conditions the licensed code can be redistributed or modified. As of Today, it is unclear whether the code generated by DL models trained on open source code should be considered as new'' or asderivative'' work, with possible implications on license infringements. In this work, we run a large-scale study investigating the extent to which DL models tend to clone code from their training set when recommending code completions. Such an exploratory study can help in assessing the magnitude of the potential licensing issues mentioned before: If these models tend to generate new code that is unseen in the training set, then licensing issues are unlikely to occur. Otherwise, a revision of these licenses urges to regulate how the code generated by these models should be treated when used, for example, in a commercial setting. Highlights from our results show that ~10% to ~0.1% of the predictions generated by a state-of-the-art DL-based code completion tool are Type-1 clones of instances in the training set, depending on the size of the predicted code. Long predictions are unlikely to be cloned.","2022-05-04T03:25:42Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set? - Matteo Ciniselli, Luca Pascarella, Gabriele Bavota",""
"2022-05-18T17:14:17Z","2022-05-18T17:21:26Z","2024-03-21T14:46:32Z","","1ab8717b-1cfa-418c-9599-e1e8a215ef4f@conf.researchr.org","","2022-04-26T06:44:26Z","To reduce technical debt and make code more maintainable, it is important to be able to warn programmers about code smells. State-of-the-art code small detectors use deep learners, without much exploration of alternatives within that technology. \nOne promising alternative for software analytics and deep learning is “GHOST” that relies on a combination of hyper-parameter optimization of feedforward neural networks and a novel oversampling technique to deal with class imbalance. \nThe prior study from TSE’21 proposing this novel “fuzzy sampling” was somewhat limited in that the method was tested on defect prediction, but nothing else. Like defect prediction, code smell detection datasets have a class imbalance (which motivated “fuzzy sampling”). Hence, in this work we test if fuzzy sampling is useful for code smell detection. \nThe results of this paper show that we can achieve better than state-of-the-art results on code smell detection with fuzzy oversampling. For example, for “feature envy”, we were able to achieve 99+% AUC across all our datasets, and on 8/10 datasets for “misplaced class” While our specific results refer to code smell detection, they do suggest other lessons for other kinds of analytics. For example: (a) try better preprocessing before trying complex learners (b) include simpler learners as a baseline in software analytics (c) try “fuzzy sampling” as one such baseline.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] How to Improve Deep Learning for Software Analytics (a case study with code smell detection) - Rahul Yedida, Tim Menzies",""
"2022-05-18T17:21:26Z","2022-05-18T17:28:34Z","2024-03-21T14:46:32Z","","7bafe77b-4151-40da-b202-19a856b1d82b@conf.researchr.org","","2022-04-26T06:44:26Z","Modern software is incredibly complex. A typical application may comprise hundreds or thousands of reusable components. Auto-mated package managers can help to maintain a consistent set of dependency versions, but ultimately the solvers in these systems rely on constraints generated by humans. At scale, small errors add up, and it becomes increasingly difficult to find high-fidelity configurations. We cannot test all configurations, because the space is combinatorial, so exhaustive exploration is infeasible.In this paper, we present Reliabuild, an auto-tuning framework that efficiently explores the build configuration space and learns which package versions are likely to result in a successful configuration. We implement two models in Reliabuild to rank the different configurations and use adaptive sampling to select good configurations with fewer samples. We demonstrate the effectiveness of Reliabuildby evaluating 31,186 build configurations of 61 packages from the Extreme-scale Scientific Software Stack (E4S), and we show that Reliabuild selects good configurations efficiently. For example,Reliabuildselects3×the number of good configurations in comparison to random sampling for several packages including Abyss, Bolt, libnrm, OpenMPI. Our framework is also able to select all the high fidelity builds in half the number of samples required by random sampling for packages such as Chai, OpenMPI,py-petsc4py, and slepc. We further use the model to learn statistics about the compatibility of different packages, which will enable package solvers to better select high-fidelity build configurations automatically.","2022-05-18T07:30:36Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Using Active Learning to Find High-Fidelity Builds - Harshitha Menon, Konstantinos Parasyris, Todd Gamblin, Tom Scogland",""
"2022-05-18T17:28:00Z","2022-05-18T17:32:00Z","2024-03-21T14:46:32Z","","08dfbc8e-65fe-49eb-8a7b-50acae9d6834@conf.researchr.org","","2022-04-26T06:44:26Z","In this paper, we present ApacheJIT, a large dataset for Just-In-Time defect prediction. ApacheJIT consists of clean and bug-inducing software changes in popular Apache projects. ApacheJIT has a total of 106,674 commits (28,239 bug-inducing and 78,435 clean commits). Having a large number of commits makes ApacheJIT a suitable dataset for machine learning models, especially deep learning models that require large training sets to effectively generalize the patterns present in the historical data to future data.","2022-05-04T03:26:32Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction - Hossein Keshavarz, Mei Nagappan",""
"2022-05-18T17:32:00Z","2022-05-18T17:36:00Z","2024-03-21T14:46:32Z","","09a27f64-6a85-4b59-ac94-6c987e7e8bd1@conf.researchr.org","","2022-04-26T06:44:26Z","It is recognized in the literature that finding representative data to conduct regression testing research is non-trivial. In our experience within this field, existing datasets are often affected by issues that limit their applicability. Indeed, these datasets often lack fine-grained coverage information, reference software repositories that are not available anymore, or do not allow researchers to readily build and run the software projects, e.g., to obtain additional information. As a step towards better replicability and data-availability in regression testing research, we introduce ReCover, a dataset of 114 pairs of subsequent versions from 28 open source Java projects from GitHub. In particular, ReCover is intended as a consolidation and enrichment of recent dedicated regression testing datasets proposed in the literature, to overcome some of the above described issues, and to make them ready to use with a broader number of regression testing techniques. To this end, we developed a custom mining tool, that we make available as well, to automatically process two recent, massive regression testing datasets, retaining pairs of software versions for which we were able to (1) retrieve the full source code; (2) build the software in a general-purpose Java/Maven environment (which we provide as a Docker container for ease of replication); and (3) compute fine-grained test coverage metrics. ReCover can be readily employed in regression testing studies, as it bundles in a single package full, buildable source code and detailed coverage reports for all the projects. We envision that its use could foster regression testing research, improving replicability and long-term data availability.","2022-05-04T03:27:27Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] ReCover: a Curated Dataset for Regression Testing Research - Francesco Altiero, Anna Corazza, Sergio Di Martino, Adriano Peron, Luigi Libero Lucio Starace",""
"2022-05-18T17:36:00Z","2022-05-18T17:50:00Z","2024-03-21T14:46:32Z","","1f91f337-2c36-4dd9-861d-af460b3405ba@conf.researchr.org","","2022-05-04T03:25:42Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-19T01:00:00Z","2022-05-19T01:07:00Z","2024-03-21T14:46:32Z","","bc2e75f1-7b47-4d7a-afe8-7e414c6a5cfa@conf.researchr.org","","2022-04-26T06:48:12Z","Human values such as integrity, privacy, curiosity, security, and honesty are guiding principles for what people consider important in life. Such human values may be violated by mobile software applications (apps), and the negative effects of such human value violations can be seen in various ways in society. In this work, we focus on the human value of honesty. We present a model to support the automatic identification of violations of the value of honesty from app reviews from an end-user perspective. Beyond the automatic detection of honesty violations by apps, we also aim to better understand different categories of honesty violations expressed by users in their app reviews. The result of our manual analysis of our honesty violations dataset shows that honesty violations can be characterised into ten categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. Based on these results, we argue for a conscious effort in developing more honest software artefacts including mobile apps, and the promotion of honesty as a key value in software development practices. Furthermore, we discuss the role of app distribution platforms as enforcers of ethical systems supporting human values, and highlight some proposed next steps for human values in software engineering (SE) research.","2022-05-04T03:52:29Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] On the Violation of Honesty in Mobile Apps: Automated Detection and Categories - Humphrey Obie, Idowu Oselumhe Ilekura, Hung Du, Mojtaba Shahin, John Grundy, Li Li, Jon Whittle, Burak Turhan",""
"2022-05-19T01:07:00Z","2022-05-19T01:14:00Z","2024-03-21T14:46:32Z","","ae4365e7-e2e8-4acc-b53d-12239729c462@conf.researchr.org","","2022-04-26T06:48:12Z","Although issues are created to discuss and solve technical problems, conversations can get heated, with discussants getting angry and/or excited for a variety of reasons, such as poor suggestions or even, the violation of community conventions. To prevent and mitigate discussions from getting heated, communities like GitHub have introduced the ability to lock issue discussions that violate the code of conduct or other community guidelines. Despite some early research on locked issues, there is a lack of understanding of how communities use this feature and of potential threats to validity for researchers relying on a dataset of locked issues as an oracle for heated discussions. To address this gap, we (i) quantitatively analyzed 79 GitHub projects that have at least one issue locked as too heated, and (ii) qualitatively analyzed all issues locked as too heated of the 79 projects, a total of 205 issues and 5,511 comments. We found that projects have different behaviors when locking issues: 14 projects locked more than 90% of their closed issues, 54 locked less than 10% of their closed issues, and 11 locked between 54% and 88% of their closed issues. Additionally, locked issues tend to have more comments and more participants compared to non-locked issues. For the 205 issues locked as too heated, we found that one-third did not contain any uncivil discourse, and only 8.82% of the analyzed comments are actually uncivil. Finally, we found that the locking justifications provided by maintainers do not always match the label used to lock the issue. Based on our results, we identify three pitfalls to avoid when using the GitHub locked issues data.","2022-05-05T04:11:35Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] How heated is it? Understanding GitHub locked issues - Isabella Ferreira, Bram Adams, Jinghui Cheng",""
"2022-05-19T01:14:00Z","2022-05-19T01:18:00Z","2024-03-21T14:46:32Z","","a612749e-d4f4-44a8-90ca-dc834922b30d@conf.researchr.org","","2022-04-26T06:48:12Z","Communication surrounding the development of an open source project largely occurs outside the software repository itself. Historically, large communities often used a collection of mailing lists to discuss the different aspects of their projects. Multimodal tool use, with software development and communication happening on different channels, complicates the study of open source projects as a sociotechnical system. Here, we combine and standardize mailing lists of the Python community, resulting in 954,287 messages from 1995 to the present. We share all scraping and cleaning code to facilitate reproduction of this work, as well as smaller datasets for the Golang (122,721 messages), Angular (20,041 messages) and Node.js (12,514 messages) communities. To showcase the usefulness of these data, we focus on the CPython repository and merge the technical layer (which GitHub account works on what file and with whom) with the social layer (messages from unique email addresses) by identifying 33% of GitHub contributors in the mailing list data. We then explore correlations between the valence of social messaging and the structure of the collaboration network. We discuss how these data provide a laboratory to test theories from standard organizational science in large open source projects.","2022-05-19T01:52:30Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] The OCEAN mailing list data set: Network analysis spanning mailing lists and code repositories - Melanie Warrick, Samuel F. Rosenblatt, Jean-Gabriel Young, amanda casari, Laurent Hébert-Dufresne, James P. Bagrow",""
"2022-05-19T01:18:00Z","2022-05-19T01:22:00Z","2024-03-21T14:46:32Z","","949196cb-5d53-485f-9884-aede71dba032@conf.researchr.org","","2022-04-26T06:48:12Z","Phabricator is a modern code collaboration tool used by popular projects like FreeBSD and Mozilla. However, unlike the other well-known code review environments, such as Gerrit or GitHub, there does not exist a readily accessible public dataset of Phabricator. This paper describes our experience mining code reviews from five different projects that use Phabricator (Blender, FreeBSD, KDE, LLVM, and Mozilla) and discusses the problems associated with the data retrieval process. We publish a dataset with details of 317,476 code reviews conducted via Phabricator. Our dataset is available in both JSON and MySQL database dump formats. The dataset enables analyses of the history of code reviews at a finer granular level than is possible on the other platforms. In addition, given that the projects we mined are accessible via the Conduit API, our dataset can be used as a foundation to fetch additional details and insights.","2022-05-14T08:53:41Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] The Unexplored Treasure Trove of Phabricator Code Reviews - Gunnar Kudrjavets, Nachiappan Nagappan, Ayushi Rastogi",""
"2022-05-19T01:22:00Z","2022-05-19T01:26:00Z","2024-03-21T14:46:32Z","","8358d04f-a5c1-4597-80c2-4c7340fde73d@conf.researchr.org","","2022-04-26T06:48:12Z","Talks at practitioner-focused open-source software conferences are a valuable source of information for software engineering researchers. They provide a pulse of the community and are valuable source material for grey literature analysis. We curated a dataset of 24,669 talks from 87 open-source conferences between 2010 and 2021. We stored all relevant metadata from these conferences and provide scripts to collect the transcripts. We believe this data is useful for answering many kinds of questions, such as: What are the important/highly discussed topics within practitioner communities? How do practitioners interact? And how do they present themselves to the public? We demonstrate the usefulness of this data by reporting our findings from two small studies: a topic model analysis providing an overview of open-source community dynamics since 2011 and a qualitative analysis of a smaller community-oriented sample within our dataset to gain a better understanding of why contributors leave open-source.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] The Unsolvable Problem or the Unheard Answer? A Dataset of 24,669 Open-Source Software Conference Talks - Kimberly Truong, Courtney Miller, Bogdan Vasilescu, Christian Kästner",""
"2022-05-19T01:26:00Z","2022-05-19T01:30:00Z","2024-03-21T14:46:32Z","","efe8bb60-afee-4f92-89e8-3a9502c36eb1@conf.researchr.org","","2022-04-26T06:48:12Z","Open Source Software (OSS) is a major component of our digital infrastructure, yet more than 80% of such projects fail. Seeking less uncertainty, many OSS projects join established software communities, e.g., the Apache Software Foundation (ASF), with established rules and community support to guide projects toward sustainability. In their nascent stage, ASF projects are incubated in the ASF incubator (ASFI), which provides systematic mentorship toward long-term sustainability. Projects in ASFI eventually conclude their incubation by either graduating, if successful, or retiring, if not. \nTime-stamped traces of developer activities are publicly available from ASF, and can be used for monitoring project trajectories toward sustainability. Here we present a web app dashboard tool, APEX, that allows internal and external stakeholders to monitor and explore ASFI project sustainability trajectories, including social and technical networks.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Exploring Apache Incubator Project Trajectories with APEX - Anirudh Ramchandran, Likang Yin, Vladimir Filkov",""
"2022-05-19T01:30:00Z","2022-05-19T01:37:00Z","2024-03-21T14:46:32Z","","c52d95f6-811f-463a-94a6-25807dfa81c2@conf.researchr.org","","2022-04-26T06:48:12Z","Happy developers are productive developers. Productivity Engineer Brian Houck will share a practitioner’s perspective on how engineering teams across Microsoft are investing in employee wellbeing to make a measurable impact on developer productivity. Measuring developer productivity across multiple dimensions by using a mix of methods, Houck takes a human-centered approach to understanding and maximizing productivity and happiness in his organization. This talk will explore real-world examples of a human centered approach improving the productivity of development teams within Microsoft. How can developers balance their time between collaboration and focused individual work? What impact does social connectedness have on the onboarding of new software engineers? Can more days off actually result in more work getting done? Learn answers to these questions and more!","2022-05-04T03:54:08Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] A Culture of Productivity: Maximizing Productivity by Maximizing Wellbeing - Brian Houck",""
"2022-05-19T01:37:00Z","2022-05-19T01:50:00Z","2024-03-21T14:46:32Z","","21a75bfa-4950-4189-b9d2-5a12cddb88f7@conf.researchr.org","","2022-05-04T03:51:59Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-18T17:00:00Z","2022-05-18T17:50:00Z","2024-03-21T14:46:32Z","","c5094a66-3925-47a6-8048-6b0be3e14118@conf.researchr.org","","2022-04-26T07:05:30Z","The purpose of scholarly peer review is to evaluate the quality of scientific manuscripts. However, study after study demonstrates that peer review neither effectively nor reliably assesses research quality. Empirical standards attempt to address this problem by modelling a scientific community’s expectations for each kind of empirical study conducted in that community. This should enhance not only the quality of research but also the reliability and predictability of peer review, as scientists adopt the standards in both their researcher and reviewer roles. However, these improvements depend on the quality and adoption of the standards. This tutorial will therefore present the empirical standard for mining software repositories, both to communicate its contents and to get feedback from the attendees. The tutorial will be organized into three parts: (1) brief overview of the empirical standards project; (2) detailed presentation of the repository mining standard; (3) discussion and suggestions for improvement.","2022-04-26T07:05:30Z","MSR Tutorials room - , , ","","","[MSR Technical Papers] Empirical Standards for Repository Mining - Paul Ralph, Tushar Sharma, Preetha Chatterjee",""
"2022-05-18T18:00:00Z","2022-05-18T18:50:00Z","2024-03-21T14:46:32Z","","9442e105-d7b4-4995-af00-93d3d5fe84fa@conf.researchr.org","","2022-04-26T07:06:23Z","","2022-04-26T07:06:23Z","MSR Tutorials room - , , ","","","[MSR Technical Papers] Mining the Ethereum Blockchain Platform: Best Practices and Pitfalls - Gustavo A. Oliva",""
"2022-05-19T08:00:00Z","2022-05-19T08:04:00Z","2024-03-21T14:46:32Z","","65d2980d-d7cb-4ca0-9947-cfa0a1900244@conf.researchr.org","","2022-04-26T06:50:10Z","Usage of the execution stack at run-time captures the dynamic state of programs and can be used to derive useful insights into the program behaviour. The stack usage information can be used to identify and debug performance and security aspects of applications. Binary run-time instrumentation techniques are well known to capture the memory access traces during program execution. Tracing the program in entirety and filtering out stack specific accesses is a commonly used technique for stack related analysis. However, applying vanilla tracing techniques (using tools like Intel Pin) for multi-threaded programs has challenges such as identifying the stack areas to perform efficient run-time tracing. \nIn this paper, we introduce SniP, an open-source stack tracing framework for multi-threaded programs built around Intel’s binary instrumentation tool Pin. SniP provides a framework for efficient run-time tracing of stack areas used by multi-threaded applications by identifying the stack areas dynamically. The targeted tracing capability of SniP is demonstrated using a range of multi-threaded applications to show its efficacy in terms of trace size and time to trace. Compared to full program tracing using Pin, SniP achieves up to 75X reduction in terms of trace file size and up to 24X reduction in time to trace. SniP complements existing trace based stack usage analysis tools and we demonstrate that SniP can be easily integrated with the analysis framework through different use-cases.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] SniP: An Efficient Stack Tracing Framework for Multi-threaded Programs - Arun KP, Saurabh Kumar, Debadatta Mishra, Biswabandan Panda",""
"2022-05-19T08:04:00Z","2022-05-19T08:08:00Z","2024-03-21T14:46:32Z","","50919fd9-6f78-43d0-a790-20c521b8a007@conf.researchr.org","","2022-04-26T06:50:10Z","Software projects under version control grow with each commit, accumulating up to hundreds of thousands of commits per repository. Especially for such large projects, the traversal of a repository and data extraction for static source code analysis poses a trade-off between granularity and speed. \nWe showcase the command-line tool pyrepositoryminer that combines a set of optimization approaches for efficient traversal and data extraction from git repositories while being adaptable to third-party and custom software metrics and data extractions. The tool is written in Python and combines bare repository access, in-memory storage, parallelization, caching, change-based analysis, and optimized communication between the traversal and custom data extraction components. The tool allows for both metrics written in Python and external programs for data extraction. A single-thread performance evaluation based on a basic mining use case shows a mean speedup of 15.6x to other freely available tools across four mid-sized open source projects. A multi-threaded execution allows for load distribution among cores and, thus, a mean speedup up to 86.9x using 12 threads.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Tooling for Time- and Space-efficient git Repository Mining - Fabian Heseding, Willy Scheibel, Jürgen Döllner",""
"2022-05-19T08:08:00Z","2022-05-19T08:12:00Z","2024-03-21T14:46:32Z","","7b6ed8c2-2072-4978-b76b-aa722938e214@conf.researchr.org","","2022-04-26T06:50:10Z","Single statement bugs are one of the most important ingredients in the evaluation of modern bug detection and automatic program repair methods. By affecting only a single statement, single statement bugs represent a type of bug often overlooked by developers, while still being small enough to be detected and fixed by automatic methods. \nWith the rise of data-driven automatic repair the availability of single statement bugs at the scale of millionth of examples is more important than ever; not only for testing these methods but also for providing sufficient real world examples for training. To provide access to bug fix datasets of this scale, we are releasing two datasets called SSB-9M and TSSB-3M. \nWhile SSB-9M provides access to a collection of over 9M general single statement bug fixes from over 500K open source Python projects , TSSB-3M focuses on over 3M single statement bugs which can be fixed solely by a single statement change. To facilitate future research and empirical investigations, we annotated each bug fix with one of 20 single statement bug (SStuB) patterns typical for Python together with a characterization of the code change as a sequence of AST modifications. Our initial investigation shows that at least 40% of all single statement bug fixes mined fit at least one SStuB pattern, and that the majority of 72% of all bugs can be fixed with the same syntactic modifications as needed for fixing SStuBs.","2022-05-14T14:17:12Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] TSSB-3M: Mining single statement bugs at massive scale - Cedric Richter, Heike Wehrheim",""
"2022-05-19T08:12:00Z","2022-05-19T08:19:00Z","2024-03-21T14:46:32Z","","0767ee50-cfc5-433f-b04b-d33a88cf9dfa@conf.researchr.org","","2022-04-26T06:50:10Z","Cloud computing promises to enhance business flexibility, efficiency, scalability, and reliability. According to a recent survey from O’Reilly, cloud adoption is steadily rising across industries, with 90% of organizations using cloud computing. Not only is cloud adoption growing, but enterprises are approaching cloud migration aggressively. Often, it is incorrectly assumed that services rendered by cloud either as platform or infrastructure will itself deliver the business outcomes of improved availability, business performance, security, and efficiency provided the application delivers the functionality. This is far from true. Cloud applications must be optimally architected, tested and operated for realizing the intended business outcomes. The considerations, choices and decisions taken in each of these lifecycle phases impact technical performance and as a result business outcome. Architecture considerations include decisions on choice of cloud services to be employed, design principles to apply, configuration parameters to tune, etc. Testing considerations include simulation of load and fault conditions that closely mirror runtime conditions and mining behavior for anomalies and faults. Operations considerations include monitoring critical parameters, identifying imminent failures (a.k.a incidents), taking preventive decisions, detecting failures, and taking recovery decisions etc. The challenge for businesses is that the cloud application runtime behavior consequences of these decisions are not easily envisaged by architects, developers, testers, and operators leading to sub-optimal business outcomes, reliability, and security issues. In addition, the decisions taken by architects, testers and operators are often inconsistent and incompatible with each other further aggravating the problem. In this Industry Track Paper (single pager) we intend to present these challenges and call for industry-academic collaboration to explore mining and modeling approaches to address the challenges","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Improved Business Outcomes from Cloud Applications – using Integrated Process and Runtime Product Data Mining - Mahesh Venkataraman, Reuben George, Jeff Wilkinson",""
"2022-05-19T08:19:00Z","2022-05-19T08:26:00Z","2024-03-21T14:46:32Z","","13fed0fd-e8ba-44a8-bab3-5d7e9b1c615c@conf.researchr.org","","2022-04-29T08:28:39Z","Public cloud services have truly come of age for enterprises worldwide, with global spend forecast to grow 47.2% in 2022, reaching $397.4 billion, up from $270 billion in 2020, according to Gartner research. According to another analyst, Verified Market Research, the global Serverless architecture market size was valued at USD 7.29 billion in 2020 and is projected to reach USD 36.84 billion by 2028, growing at a CAGR of 21.71% from 2021 to 2028. With the growing complexity of cloud applications, meeting the demand of going from prototype to production to planet-scale is challenging. Serverless architectures ensure pay-as-you-go and can be invoked and scaled individually. Serverless is a method for executing functions and running cloud compute services on an as-needed basis. Serverless is the most scalable and cost-effective method for cloud computing. Enterprises that are adopting serverless architectures, can achieve their speed-to-market and business functionality goals only with a rigorous focus on quality from the start of their cloud journey and keep it up throughout. The goal? To increase quality, reduce cost and improve time-to-market across the journey to cloud. \nThe challenge : In the current process of serverless applications/functions development, the build is done locally using Serverless framework using frameworks provided by all major cloud providers like AWS SAM, Azure Functions Core Tools, etc. These frameworks allow developers to simulate the cloud services locally for development purpose. An issue with these frameworks is they only work with their respective cloud provider. E.g. AWS SAM works only with AWS. The System &amp; Integration test phase is optional and rarely performed. If it gets included the process is completely manual through the cloud console - lets assume its an AWS instance, hence Lambda console. System and Integration testing is often skipped as its time consuming since serverless functions often do not have a UI, and black-box testing is not possible. This increases the risk of defect leakage thus exponentially raising the cost of using serverless architectures. Serverless function testing is critical to mitigate that risk. \nThe Solution : Our solution is a self-service tool that aids both developers and testers. Using our tool, one can execute System &amp; Integration testing in a completely autonomous manner. This is achieved by data mining of software repositories. 1. Unit testing coverage reports mining 2. Code repository mining The solution engine is designed to mine the above repositories and detect changes, when a new code build is deployed. Based on identified changes, it generates test scripts using an automation engine. These scripts along with a synthetic data generator are pushed to be executed on the serverless lambda functions. The AI based self-healing module ensures scripts are automatically updated to provide seamless execution of automated test scripts. The solution identifies faults on the serverless lambda functions in the shortest possible time, that are usually extremely challenging to find. \nThe Benefits : • The solution business logic is separate from the FaaS provider (AWS, GCP, Azure, etc.), to keep it provider-independent and reusable across hyperscalers. • The entire process is autonomous right from test scenario creation to seamless execution along with self-healing of the scripts. • Ability to test functions triggered from other functions • Improved code and test coverage to over 90% thus leading to lower defect leakage and improved E2E cycle time. • The solution is designed to be a self-service tool which both developers and testers can use.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Improve Quality of Cloud Serverless Architectures through Software Repository Mining - Mallika Fernandes, Rohit Patwardhan",""
"2022-05-19T08:26:00Z","2022-05-19T08:30:00Z","2024-03-21T14:46:32Z","","3b298e09-48db-401a-aa87-a8e6b8af343e@conf.researchr.org","","2022-04-26T06:50:10Z","Unit testing verifies the presence of faults in individual software components. Previous research has been targeting the automatic generation of unit tests through the adoption of random or search-based algorithms. Despite their effectiveness, these approaches do not implement any strategy that allows them to create unit tests in a structured manner: indeed, they aim at creating tests by optimizing metrics like code coverage without ensuring that the resulting tests follow good design principles. In order to structure the automatic test case generation process, we propose a two-step systematic approach to the generation of unit tests: we first force search-based algorithms to create tests that cover individual methods of the production code, hence implementing the so-called intra-method tests; then, we relax the constraints to enable the creation of intra-class tests that target the interactions among production code methods.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Toward Granular Automatic Unit Test Case Generation - Fabiano Pecorelli, Giovanni Grano, Fabio Palomba, Harald C. Gall, Andrea De Lucia",""
"2022-05-19T08:30:00Z","2022-05-19T08:50:00Z","2024-03-21T14:46:32Z","","619eb9b1-2164-486d-b079-11128acc6f3b@conf.researchr.org","","2022-05-04T04:01:44Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-20T00:00:00Z","2022-05-20T00:04:00Z","2024-03-21T14:46:32Z","","b3cfb2ab-6024-4f62-b495-1cf7f27fd19a@conf.researchr.org","","2022-04-26T06:58:52Z","That open source software comprises an increasingly large percentage of modern software applications has become conventional wisdom. The exact extent to which open source software constitutes today’s applications is indeterminate, however, at least by the standards of the academic software engineering research community. This paper proposes a methodology and associated tool that can analyze Java binaries and determine the proportion of open source that compose it.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Is Open Source Eating the World’s Software? Measuring the Proportion of Open Source in proprietary software using Java Binaries - Julius Musseau, John Speed Meyers, George P. Sieniawski, C. Albert Thompson, Daniel M. German",""
"2022-05-20T00:04:00Z","2022-05-20T00:11:00Z","2024-03-21T14:46:32Z","","afb4068b-6793-48d1-91ea-cb6e784cec06@conf.researchr.org","","2022-04-26T06:58:52Z","Increasing code velocity (or the speed with which code changes are reviewed and merged) is integral to speeding up development and contributes to the work satisfaction of engineers. While factors affecting code change acceptance have been investigated in the past, solutions to decrease the code review lifetime are less understood. This study investigates the code review process to quantify delays and investigate opportunities to potentially increase code velocity.We study the temporal characteristics of half a million code reviews hosted on Gerrit and Phabricator, starting from the first response, to a decision to accept or reject the changes, and until the changes are merged into a target branch. We identified two types of time delays: (a) the wait time from the proposal of code changes until first response, and (b) the wait time between acceptance and merging. Our study indicates that reducing the time between acceptance and merging has the potential to speed up Phabricator code reviews by 29–63%. Small code changes and changes made by authors with a large number of previously accepted code reviews have a higher chance of being immediately accepted, without code review iterations. Our analysis suggests that switching from manual to automatic merges can help increase code velocity.","2022-05-14T08:53:41Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Mining Code Review Data to Understand Waiting Times Between Acceptance and Merging: An Empirical Analysis - Gunnar Kudrjavets, Aditya Kumar, Nachiappan Nagappan, Ayushi Rastogi",""
"2022-05-20T00:11:00Z","2022-05-20T00:18:00Z","2024-03-21T14:46:32Z","","ad96b697-b010-4ab8-9a5d-a896d692dc6a@conf.researchr.org","","2022-04-26T06:58:52Z","Despite decades of research, SE lacks widely accepted models (that offer precise quantitative stable predictions) about what factors most influence software quality. This paper provides a promising result showing such stable models can be generated using a new transfer learning framework called “STABILIZER”. Given a tree of recursively clustered projects (using project meta-data), STABILIZER promotes a model upwards if it performs best in the lower clusters (stopping when the promoted model performs worse than the models seen at a lower level). \nThe number of models found by STABILIZER is minimal: one for defect prediction (756 projects) and less than a dozen for project health (1628 projects). Hence, via STABILIZER, it is possible to find a few projects which can be used for transfer learning and make conclusions that hold across hundreds of projects at a time. Further, the models produced in this manner offer predictions that perform as well or better than the prior state-of-the-art. \nTo the best of our knowledge, STABILIZER is the order of magnitude faster than the prior state-of-the-art transfer learners which seek to find conclusion stability, and these case studies are the largest demonstration of the generalizability of quantitative predictions of project quality yet reported in the SE literature. \nIn order to support open science, all our scripts and data are online at https://github.com/Anonymous633671/STABILIZER.","2022-05-14T14:17:12Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Methods for Stabilizing Models across Large Samples of Projects(with case studies on Predicting Defect and Project Health) - Suvodeep Majumder, Tianpei Xia, Rahul Krishna, Tim Menzies",""
"2022-05-20T00:18:00Z","2022-05-20T00:25:00Z","2024-03-21T14:46:32Z","","f0058208-20b5-4aa6-91e9-6d3d8e52cb50@conf.researchr.org","","2022-04-26T06:58:52Z","Code velocity, or the speed with which code changes are integrated into a production environment, plays a crucial role in Continuous Integration and Continuous Deployment. Many studies report factors influencing code velocity. However, solutions to increase code velocity are unclear. Meanwhile, the industry continues to issue guidelines on “ideal” code change size, believing it increases code velocity despite lacking evidence validating the practice. Surprisingly, this fundamental question has not been studied to date. This study investigates the practicality of improving code velocity by optimizing pull request size and composition(ratio of insertions, deletions, and modifications). We start with a hypothesis that a moderate correlation exists between pull request size and time-to-merge. We selected 100 most popular, actively developed projects from 10 programming languages on GitHub. We analyzed our dataset of 845,316 pull requests by size, composition, and context to explore its relationship to time-to-merge—a proxy to measure code velocity. Our study shows that pull request size and composition do not relate to time-to-merge. Regardless of the contextual factors that can influence pull request size or composition (e.g., programming language), the observation holds. Pull request data from two other platforms: Gerrit and Phabricator (401,790 code reviews) confirms the lack of relationship. This negative result as in “… eliminate useless hypotheses … ” [73] challenges a widespread belief by showing that small code changes do not merge faster to increase code velocity.","2022-05-14T08:53:41Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Do Small Code Changes Merge Faster? A Multi-Language Empirical Investigation - Gunnar Kudrjavets, Nachiappan Nagappan, Ayushi Rastogi",""
"2022-05-20T00:25:00Z","2022-05-20T00:32:00Z","2024-03-21T14:46:32Z","","ab98474f-e587-4918-bf40-f27015be8258@conf.researchr.org","","2022-04-26T06:58:52Z","In software projects, applications are often monitored by systems that automatically identify crashes, collect their information into reports, and submit them to developers. Especially in popular applications, such systems tend to generate a large number of crash reports in which a significant portion of them are duplicate. Due to this high submission volume, in practice, the crash report deduplication is supported by devising automatic systems whose efficiency is a critical constraint. In this paper, we focus on improving deduplication system throughput by speeding up the stack trace comparison. In contrast to the state-of-the-art techniques, we propose FaST, a novel sequence alignment method that computes the similarity score between two stack traces in linear time. Our method independently aligns identical frames in two stack traces by means of a simple alignment heuristic. We evaluate FaST and five competing methods on four datasets from open-source projects using ranking and binary metrics. Despite its simplicity, FaST consistently achieves state-of-the-art performance regarding all metrics considered. Moreover, our experiments confirm that FaST is substantially more efficient than methods based on optimal sequence alignment.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] FaST: A linear time stack trace alignment heuristic for crash report deduplication - Irving Muller Rodrigues, Daniel Aloise, Eraldo Rezende Fernandes",""
"2022-05-20T00:32:00Z","2022-05-20T00:36:00Z","2024-03-21T14:46:32Z","","76c67648-14d6-4b57-bdbc-75e9f2e172ee@conf.researchr.org","","2022-04-26T06:58:52Z","System call traces are an invaluable source of information about a program’s runtime behavior, and have been shown to be particularly useful for malware detection in Android apps. However, the paucity of publicly available high quality dataset hinders the development of the field. In this paper, we introduce TwinDroid, a dataset of over 1000 system calls traces, from both benign and infected Android apps. A large part of the dataset is composed of traces from pairs benign and infected apps, identical apart from the inclusion of malware in the latter. This makes TwinDroid an ideal basis for security research, and an earlier version of TwinDroid has already been used for this purpose. In addition to a dataset of traces, TwinDroid includes a fully automated traces generation pipeline, which allows users to seamlessly generate new traces in a standardized manner. This pipeline will allow the dataset to remain up-to-date and relevant despite the rapid pace of change that characterizes Android security.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] TwinDroid: A Dataset of Android app System call traces and Trace Generation Pipeline - Asma Razgallah, Raphael Khoury, Jean-Baptiste Poulet",""
"2022-05-20T00:36:00Z","2022-05-20T00:50:00Z","2024-03-21T14:46:32Z","","111d42c8-05a7-47d2-bf72-c4014fccde22@conf.researchr.org","","2022-05-04T03:42:08Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-19T07:00:00Z","2022-05-19T07:07:00Z","2024-03-21T14:46:32Z","","94b84f8e-11a3-44cb-ac07-789b3389653c@conf.researchr.org","","2022-04-26T06:49:17Z","The number of Open Source projects hosted in social coding platforms such as GitHub is constantly growing. However, many of these projects are not regularly maintained and some are even abandoned shortly after they were created. In this paper we analyze early project development dynamics in software projects hosted on GitHub, including their survival rate. To this aim, we collected all 1,127 GitHub repositories from four different ecosystems (i.e., NPM packages, R packages, WordPress plugins and Laravel packages) created in 2016. We stored their activity in a time series database and analyzed their activity evolution along their lifespan, from 2016 to now. Our results reveal that the prototypical development process consists of intensive coding-driven active periods followed by long periods of inactivity. More importantly, we have found that a significant number of projects die in the first year of existence with the survival rate decreasing year after year. In fact, the probability of surviving longer than five years is less than 50% though some types of projects have better chances of survival.","2022-05-04T03:57:50Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] An Empirical Study on the Survival Rate of GitHub Projects - Adem Ait-Fonolla, Javier Luis Cánovas Izquierdo, Jordi Cabot",""
"2022-05-19T07:07:00Z","2022-05-19T07:14:00Z","2024-03-21T14:46:32Z","","28c2f57a-f181-4d13-888b-2415833a1e06@conf.researchr.org","","2022-04-26T06:49:17Z","In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. \nIn this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.","2022-05-04T03:57:59Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts - Konstantin Grotov, Sergey Titov, Vladimir Sotnikov, Yaroslav Golubev, Timofey Bryksin",""
"2022-05-19T07:14:00Z","2022-05-19T07:21:00Z","2024-03-21T14:46:32Z","","aacc506e-c7c7-44f4-8c62-ae410c2a3fed@conf.researchr.org","","2022-04-26T06:49:17Z","To satisfy varying customer needs, device vendors and OS providers often rely on the open-source nature of the Android OS and offer customized versions of the Android OS. When a new version of the Android OS is released, device vendors and OS providers need to merge the changes from the Android OS into their customizations to account for its bug fixes, security patches, and new features. Because developers of customized OSs might have made changes to code locations that were also modified by the developers of the Android OS, the merge task can be characterized by conflicts, which can be time-consuming and error-prone to resolve. \nTo provide more insight into this critical aspect of the Android ecosystem, we present an empirical study that investigates how eight open-source customizations of the Android OS merge the changes from the Android OS into their projects. The study analyzes how often the developers from the customized OSs merge changes from the Android OS, how often the developers experience textual merge conflicts, and the characteristics of these conflicts. Furthermore, to analyze the effect of the conflicts, the study also analyzes how the conflicts can affect a randomly selected sample of 1,000 apps. After analyzing 1,148 merge operations, we identified that developers perform these operations for 9.7% of the released versions of the Android OS, developers will encounter at least one conflict in 41.3% of the merge operations, 58.1% of the conflicts required developers to change the customized OSs, and 64.4 of the apps considered use at least one method affected by a conflict. In addition to detailing our results, the paper also discusses the implications of our findings and provides insights for researchers and practitioners working with Android and its customizations.","2022-05-04T03:58:08Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Do Customized Android Frameworks Keep Pace with Android? - Pei Liu, Mattia Fazzini, John Grundy, Li Li",""
"2022-05-19T07:21:00Z","2022-05-19T07:25:00Z","2024-03-21T14:46:32Z","","186a6aab-d28d-4611-a74f-8c7aa0e0d831@conf.researchr.org","","2022-04-26T06:49:17Z","In this paper, we present Lupa - a framework for large-scale analysis of the programming language usage. Lupa is a command line tool that uses the power of the IntelliJ Platform under the hood, which gives it access to powerful static analysis tools used in modern IDEs. The tool supports custom analyzers that process the rich concrete syntax tree of the code and can calculate its various features: the presence of entities, their dependencies, definition-usage chains, etc. Currently, Lupa supports analyzing Python and Kotlin, but can be extended to other languages supported by IntelliJ-based IDEs. We explain the internals of the tool, show how it can be extended and customized, and describe an example analysis that we carried out with its help: analyzing the syntax of ranges in Kotlin.","2022-05-04T03:58:25Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Lupa: A Platform for Large Scale Analysis of The Progamming Language Usage - Anna Vlasova, Maria Tigina, Ilya Vlasov, Anastasiia Birillo, Yaroslav Golubev, Timofey Bryksin",""
"2022-05-19T07:25:00Z","2022-05-19T07:29:00Z","2024-03-21T14:46:32Z","","bff55226-6eaf-44e1-bbd7-313506d6088a@conf.researchr.org","","2022-04-29T08:24:28Z","Conducting socio-technical software engineering research on closed-source software is difficult as most organizations do not want to give access to their code repositories. Most experiments and publications therefore focus on open-source projects which only provides a partial view of software development communities. Yet, closing the gap between open and closed source software industries is essential to increase the validity and applicability of results stemming from socio-technical software engineering research. We contribute to this effort by sharing our work in a large company counting 4,800 employees. We mined 101 repositories and produced the GDED dataset containing socio-technical information about 106,216 commits, 470,940 file modifications and 3,471,556 method modifications from 164 developers during the last 13 years, using various programming languages. For that, we used GitDelver, an open-source tool we developed on top of Pydriller, and anonymized and scrambled the data to comply with legal and corporate requirements. Our dataset can be used for various purposes and provides information about code complexity, self-admitted technical debt, bug fixes, as well as temporal information. We also share our experience regarding the processing of sensitive data to help other organizations making datasets publicly available to the research community.","2022-05-04T03:58:38Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] GitDelver Enterprise Dataset (GDED): An Industrial Closed-source Dataset for Socio-Technical Research - Nicolas Riquet, Xavier Devroey, Benoît Vanderose",""
"2022-05-19T07:29:00Z","2022-05-19T07:33:00Z","2024-03-21T14:46:32Z","","c03dade2-7a61-411e-ac47-d2e07892dd52@conf.researchr.org","","2022-04-26T06:49:17Z","Software package managers facilitate reuse and rapid construc- tion of software systems. Since evermore software is distributed via package managers, researchers and practitioners require ex- plicit data of software dependency networks that are opaquely formed by dependency relations between software packages. To reason about increasingly complex software products and ecosys- tems, researchers and practitioners rely either on publicly available datasets like the seemingly unattended libraries.io [15] or they mine problem-specific data from software ecosystems repeatedly and non-transparently. Therefore, we present the DaSEA dataset, which contains metadata of software packages, their versions, and de- pendencies from multiple ecosystems (currently six programming languages and five operating system package managers). Alongside the dataset, we provide an extensible open-source tool under the same name that is used to create updated versions of the DaSEA dataset allowing studies of evolution of software ecosystems.","2022-05-16T09:45:24Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] DaSEA – A Dataset for Software Ecosystem Analysis - Petya Buchkova, Joakim Hey Hinnerskov, Kasper Olsen, Rolf-Helge Pfeiffer",""
"2022-05-19T07:33:00Z","2022-05-19T07:37:00Z","2024-03-21T14:46:32Z","","215a4296-2aad-43ef-b2eb-51e1c30a1fd4@conf.researchr.org","","2022-04-26T06:49:17Z","Third party libraries are used to integrate existing solutions for common problems and help speed up development. The use of third party libraries, however, can carry risks, for example through vulnerabilities in these libraries. Studying the dependency networks of package managers lets us better understand and mitigate these risks. So far, the dependency networks of the three most important package managers of the Apple ecosystem, CocoaPods, Carthage and Swift PM, have not been studied. We analysed the dependencies for all publicly available open source libraries up to December 2021 and compiled a dataset containing the dependency networks of all three package managers. The dependency networks can be used to analyse how vulnerabilities are propagated through transitive dependencies. In order to ease the tracing of vulnerable libraries we also queried the NVD database and included publicly reported vulnerabilities for these libraries in the dataset.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Dataset: Dependency Networks of Open Source Libraries Available Through CocoaPods, Carthage and Swift PM - Kristiina Rahkema, Dietmar Pfahl",""
"2022-05-19T07:37:00Z","2022-05-19T07:50:00Z","2024-03-21T14:46:32Z","","00779063-9633-477e-8ddf-7117cb4cca69@conf.researchr.org","","2022-05-04T03:57:35Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-19T09:00:00Z","2022-05-19T09:04:00Z","2024-03-21T14:46:32Z","","45889aed-cec4-4469-aaf2-576bff2e1348@conf.researchr.org","","2022-04-26T06:54:23Z","Speeding up development may produce technical debt, i.e., not-quite-right code for which the effort to make it right increases with time as a sort of interest. Developers may be aware of the debt as they admit it in their code comments. Literature reports that such a self-admitted technical debt survives for a long time in a program, but it is not yet clear its impact on the quality of the code on the long term. We argue that self-admitted technical debt contains a number of different weaknesses that may affect the security of a program. Therefore, the longer a debt is not paid back the higher is the risk that the weaknesses can be exploited. To discuss our claim and rise the developers’ awareness on the vulnerability of the self-admitted technical debt that are not paid back, we explore the self-admitted technical debt in the Chromium C-code to detect any known weaknesses. In this preliminary study, we first mine the Common Weakness Enumeration repository to define heuristics for the automatic detection and fix of weak code. Then, we parse the C-code to find self-admitted technical debt and the code block it refers to. Finally, we use the heuristics to find weak code snippets associated to self-admitted technical debt and recommend their potential mitigation to developers. Such knowledge can be used to prioritize self-admitted technical debt for repair. A prototype has been developed and applied to the Chromium code. Initial findings report that 55% of self-admitted technical debt code contains weak code of 14 different types.","2022-05-09T18:21:51Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] WeakSATD: detecting weak self-admitted technical debt - Barbara Russo, Matteo Camilli, Moritz Mock",""
"2022-05-19T09:04:00Z","2022-05-19T09:11:00Z","2024-03-21T14:46:32Z","","b08bf682-e21a-4285-bb30-4c344cf21ad4@conf.researchr.org","","2022-04-26T06:54:23Z","Third-party libraries (TPLs) are reused frequently in software applications for reducing development cost. However, they could introduce security risks as well. Many TPL detection methods have been proposed to detect TPL reuse in Android bytecode or in source code. This paper focuses on detecting TPL reuse in binary code, which is a more challenging task. For a detection target in binary form, libraries may be compiled and linked to separate dynamic-link files or built into a fused binary that contains multiple libraries and project-specific code. This could result in fewer available code features and lower the effectiveness of feature engineering. In this paper, we propose a binary TPL reuse detection framework, LibDB, which can effectively and efficiently detect imported TPLs even in stripped and fused binaries. In addition to the basic and coarse-grained features(string literals and exported function names), LibDB utilizes function contents as a new type of feature. It embeds all functions in a binary file to low-dimensional representations with a trained neural network. It further adopts a function call graph-based comparison method to improve the accuracy of the detection. LibDB is able to support version identification of TPLs contained in the detection target, which is not considered by existing detection methods. To evaluate the performance of LibDB, we construct three datasets for binary-based TPL reuse detection. Our experimental results show that LibDB is more accurate and efficient than state-of-the-art tools on the binary TPL detection task and the version identification task. Our datasets and source code used in this work are anonymously available at https://anonymous.4open.science/r/LibDB.","2022-05-09T16:55:52Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] LibDB: An Effective and Efficient Framework for Detecting Third-Party Libraries in Binaries - Wei Tang, Yanlin Wang, Hongyu Zhang, Shi Han, Ping Luo, Dongmei Zhang",""
"2022-05-19T09:11:00Z","2022-05-19T09:18:00Z","2024-03-21T14:46:32Z","","430dc07b-713b-40d1-80e7-031943408616@conf.researchr.org","","2022-04-26T06:54:23Z","Data-driven software engineering processes, such as vulnerability prediction heavily rely on the quality of the data used. In this paper, we observe that noise-free security defect datasets are infeasible to be obtained in practice. Despite the vulnerable class, the non-vulnerable modules are difficult to be verified and determined as truly exploit free given the limited manual efforts available. It results in uncertainty, introducing labeling noise in the datasets and affecting conclusion validity. To address this issue, we propose novel learning methods that are robust to label impurities and can leverage the most from limited label data; noisy label learning. We investigate various noisy label learning methods applied to software vulnerability prediction. Specifically, we propose a two-stage learning method based on noise cleaning to identify and remediate the noisy samples, which improves AUC and recall of baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several hurdles in terms of achieving a performance upper bound with semi-omniscient knowledge of the label noise. Overall, the experimental results show that learning from noisy labels can be effective for data-driven software and security analytics.","2022-05-04T04:21:45Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Noisy Label Learning for Security Defects - Roland Croft, Muhammad Ali Babar, Huaming Chen",""
"2022-05-19T09:18:00Z","2022-05-19T09:22:00Z","2024-03-21T14:46:32Z","","66a88256-223a-4c44-8cda-1d19c1f925bb@conf.researchr.org","","2022-04-26T06:54:23Z","In this work we present Vul4J, a Java vulnerability dataset where each vulnerability is associated to a patch and, most importantly, to a Proof of Vulnerability (PoV) test case. We analyzed 1803 fix commits from 912 real-world vulnerabilities in the Project KB knowledge base to extract the reproducible vulnerabilities, i.e., vulnerabilities that can be triggered by one or more PoV test cases. To this aim, we ran the test suite of the application in both, the vulnerable and secure versions, to identify the corresponding PoVs. Furthermore, if no PoV test case was spotted, then we wrote it ourselves. As a result, Vul4J includes 79 reproducible vulnerabilities from 51 open-source projects, spanning 25 different Common Weakness Enumeration (CWE) types. To the extent of our knowledge, this is the first dataset of its kind created for Java. Particularly, it targets the study of Automated Program Repair (APR) tools, where PoVs are often necessary in order to identify plausible patches. We made our dataset and related tools publically available on GitHub.","2022-05-10T03:34:46Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Vul4J: A Dataset of Reproducible Java Vulnerabilities Geared Towards the Study of Program Repair Techniques - Quang-Cuong Bui, Riccardo Scandariato, Nicolás E. Díaz Ferreyra",""
"2022-05-19T09:22:00Z","2022-05-19T09:26:00Z","2024-03-21T14:46:32Z","","2cad44b3-7c6c-4620-a510-f723bdf2f8be@conf.researchr.org","","2022-04-26T06:54:23Z","With the large-scale adaptation of Android OS and ever-increasing contributions in the Android application space, Android has become the number one target of malware writers. In recent years, a large number of automatic malware detection and classification systems have evolved to tackle the dynamic nature of malware growth using either static or dynamic analysis techniques. Performance of static malware detection methods degrade due to the obfuscation attacks. Although many benchmark datasets are available to measure the performance of malware detection and classification systems, only a single obfuscated malware dataset (PRAGuard) is available to showcase the efficacy of the existing malware detection systems against the obfuscation attacks. PRAGuard contains outdated samples till March 2013 and does not represent the latest application categories. Moreover, PRAGuard does not provide the family information for malware because of which PRAGuard can not be used to evaluate the efficacy of the malware family classification systems. \nIn this work, we create and release AndroOBFS, a time-tagged (at month granularity) obfuscated malware dataset with familial information spanning over three years from 2018 to 2020. We create this dataset by obfuscating 16279 unique real-world malware in six different obfuscation categories. Out of 16279 obfuscated malware samples, 14579 samples are distributed across 158 families with at least two unique malware samples in each family. We release this dataset to facilitate Android malware study towards designing robust and obfuscation resilient malware detection and classification systems.","2022-06-29T12:50:26Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] AndroOBFS: Time-tagged Obfuscated Android Malware Dataset with Family Information - Saurabh Kumar, Debadatta Mishra, Biswabandan Panda, Sandeep K. Shukla",""
"2022-05-19T09:26:00Z","2022-05-19T09:30:00Z","2024-03-21T14:46:32Z","","b0a6de24-d64a-40e9-99f9-3d89be6a4b74@conf.researchr.org","","2022-04-26T06:54:23Z","Many Android apps analyzers rely, among other techniques, on dynamic analysis to monitor their runtime behavior and detect potential security threats. However, malicious developers use subtle, though efficient, techniques to bypass dynamic analyzers. Logic bombs are examples of popular techniques where the malicious code is triggered only under specific circumstances, challenging comprehensive dynamic analyses. The research community has proposed various approaches and tools to detect logic bombs. Unfortunately, rigorous assessment and fair comparison of state-of-the-art techniques are impossible due to the lack of ground truth. In this paper, we contribute with TriggerZoo, a new dataset of 406 Android apps containing logic bombs and benign trigger-based behavior that we release only to the research community using authenticated API. These apps are real-world apps from Google Play that have been automatically infected by our tool AndroBomb. The injected pieces of code implementing the logic bombs cover a large pallet of realistic logic bomb types that we have manually characterized from a set of real logic bombs. Researchers can exploit this dataset as ground truth to assess their approaches and provide comparisons against other tools.","2022-05-05T00:44:56Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] TriggerZoo: A Dataset of Android Applications Automatically Infected with Logic Bombs - Jordan Samhi, Tegawendé F. Bissyandé, Jacques Klein",""
"2022-05-19T09:30:00Z","2022-05-19T09:34:00Z","2024-03-21T14:46:32Z","","bcc30ba3-ef3d-4654-a743-8362e5a59f9e@conf.researchr.org","","2022-04-26T06:54:23Z","Context: Cryptographic APIs are often misused in real-world applications. Therefore, many cryptographic API misuse detection tools have been introduced. However, there exists no established reference benchmark for a fair and comprehensive comparison and evaluation of these tools. While there are benchmarks, they often only address a subset of the domain or were only used to evaluate a subset of existing misuse detection tools. Objective: To fairly compare cryptographic API misuse detection tools and to drive future development in this domain, we will devise such a benchmark. Openness and transparency in the generation process are key factors to fairly generate and establish the needed benchmark. Method:We propose an approach where we derive the benchmark generation methodology from the literature which consists of general best practices in benchmarking and domain-specific benchmark generation. A part of this methodology is transparency and openness of the generation process, which is achieved by pre-registering this work. Based on our methodology we design CamBench, a fair “Cryptographic API Misuse Detection Tool Benchmark Suite”. We will implement the first version of CamBench limiting the domain to Java, the JCA, and static analyses. Finally, we will use CamBench to compare current misuse detection tools and compare CamBench to related benchmarks of its domain.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] CamBench - Cryptographic API Misuse Detection Tool Benchmark Suite - Michael Schlichtig, Anna-Katharina Wickert, Stefan Krüger, Eric Bodden, Mira Mezini",""
"2022-05-19T09:34:00Z","2022-05-19T09:50:00Z","2024-03-21T14:46:32Z","","13ed1f08-9d76-4a37-b5c1-b3c3499e9764@conf.researchr.org","","2022-05-04T03:35:29Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-19T15:00:00Z","2022-05-19T15:04:00Z","2024-03-21T14:46:32Z","","91d8687f-cecf-47f1-9b8e-1990bea50702@conf.researchr.org","","2022-04-26T06:56:54Z","Compiler fuzzing tools such as Csmith have uncovered many bugs in compilers by randomly sampling programs from a generative model. The success of these tools is often attributed to their ability to generate unexpected corner case inputs that developers tend to overlook during manual testing. At the same time, their chaotic nature makes fuzzer-generated test cases notoriously hard to interpret, which has lead to the creation of input simplification tools such as C-Reduce (for C compiler bugs). In until now unrelated work, researchers have also shown that human-written software tends to be rather repetitive and predictable to language models. Studies show that developers deliberately write more predictable code, whereas code with bugs is relatively unpredictable. In this study, we ask the natural questions of whether this high predictability property of code also, and perhaps counter-intuitively, applies to fuzzer-generated code. That is, we investigate whether fuzzer-generated compiler inputs are deemed unpredictable by a language model built on human-written code and surprisingly conclude that \\emph{it is not}. To the contrary, Csmith fuzzer-generated programs are \\emph{more} predictable on a per-token basis than human-written C programs. Furthermore, bug-triggering tended to be more predictable still than random inputs, and the C-Reduce minimization tool did not substantially increase this predictability. Rather, we find that bug-triggering inputs are unpredictable relative to \\emph{Csmith’s own} generative model. This is encouraging; our results suggest promising research directions on incorporating predictability metrics in the fuzzing and reduction tools themselves.","2022-05-14T09:36:49Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] On the Naturalness of Fuzzer Generated Code - Rajeswari Hita Kambhamettu, John Billos, Carolyn Tomi Oluwaseun-Apo, Benjamin Gafford, Rohan Padhye, Vincent J. Hellendoorn",""
"2022-05-19T15:04:00Z","2022-05-19T15:11:00Z","2024-03-21T14:46:32Z","","8b255ded-53f3-4e49-8161-8a7c608c52b2@conf.researchr.org","","2022-04-26T06:56:54Z","Learning and predicting the performance of a configurable software system helps to provide better quality assurance. One important engineering decision therein is how to encode the configuration into the model built. Despite the presence of different encoding schemes, there is still little understanding of which is better and under what circumstances, as the community often relies on some general beliefs that inform the decision in an ad-hoc manner. To bridge this gap, in this paper, we empirically compared the widely used encoding schemes for software performance learning, namely label, scaled label, and one-hot encoding. The study covers five systems, seven models, and three encoding schemes, leading to 105 cases of investigation. Our key findings reveal that: (1) conducting trial-and-error to find the best encoding scheme in a case by case manner can be rather expensive, requiring up to 400+ hours on some models and systems; (2) the one-hot encoding often leads to the most accurate results while the scaled label encoding is generally weak on accuracy over different models; (3) conversely, the scaled label encoding tends to result in the fastest training time across the models/systems while the one-hot encoding is the slowest; (4) for all models studied, label and scaled label encoding often lead to relatively less biased outcomes between accuracy and training time, but the paired model varies according to the system. \nWe discuss the actionable suggestions derived from our findings, hoping to provide a better understanding of this topic for the community. To promote open science, the data and code of this work can be publicly accessed at https://doi.org/10.5281/zenodo.5884197.","2022-05-05T00:37:45Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Does Configuration Encoding Matter in Learning Software Performance? An Empirical Study on Encoding Schemes - Jingzhi Gong, Tao Chen",""
"2022-05-19T15:11:00Z","2022-05-19T15:18:00Z","2024-03-21T14:46:32Z","","78d9462b-d812-4dd9-9df1-a9868288457e@conf.researchr.org","","2022-04-26T06:56:54Z","Collaboration platforms, such as GitHub and Slack, are a vital instrument in the day-to-day routine of software engineering teams. The data stored in these platforms has a significant value for data-driven methods that assist with decision-making and help improve software quality. However, the distribution of this data across different platforms leads to the fact that combining it is a very time-consuming process. Most existing algorithms for socio-technical assistance, such as recommendation systems, are based only on data directly related to the purpose of the algorithms, often originating from a single system. \nIn this work, we explore the capabilities of a multimodal recommendation system in the context of software engineering. Using records of interaction between employees in a software company in messenger channels and repositories, as well as the organizational structure, we build several channel recommendation models for a software engineering collaboration platform, and compare them on historical data. In addition, we implement a channel recommendation bot and assess the quality of recommendations from the best models with a user study. \nWe find that the multimodal recommender yields better recommendations than unimodal baselines, allows to mitigate the overfitting problem, and helps to deal with cold start. Our findings suggest that the multimodal approach is promising for other recommendation problems in software engineering.","2022-05-04T04:26:03Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Multimodal Recommendation of Messenger Channels - Ekaterina Koshchenko, Egor Klimov, Vladimir Kovalenko",""
"2022-05-19T15:18:00Z","2022-05-19T15:25:00Z","2024-03-21T14:46:32Z","","f6d49e9c-e569-4d8b-a8aa-600799c1d325@conf.researchr.org","","2022-04-26T06:56:54Z","Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with \\emph{Senatus}, a new code-to-code recommendation engine. At the core of Senatus is \\emph{De-Skew} LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example on the CodeSearchNet dataset Senatus improves performance by 31.21% F1 and 147.9\\emph{x} faster query time compared to Facebook Aroma. Senatus also outperforms standard MinHash LSH by 29.2% F1 and 51.02\\emph{x} faster query time.","2022-05-14T09:36:49Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Senatus: A Fast and Accurate Code-to-Code Recommendation Engine - Fran Silavong, Sean Moran, Antonios Georgiadis, Rohan Saphal, Robert Otter",""
"2022-05-19T15:25:00Z","2022-05-19T15:32:00Z","2024-03-21T14:46:32Z","","f71b4fd9-fd6a-448b-bdb6-0c0746fb17b9@conf.researchr.org","","2022-04-26T06:56:54Z","Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged but at the expense of run-time performance. While hybrid approaches aim for the “best of both worlds,” the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges—and resultant bugs—involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation—the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.","2022-07-18T20:18:33Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study - Tatiana Castro Vélez, Raffi Khatchadourian, Mehdi Bagherzadeh, Anita Raja",""
"2022-05-19T15:32:00Z","2022-05-19T15:39:00Z","2024-03-21T14:46:32Z","","b5b76e17-7d19-4fac-9c1a-c0bb43d1de68@conf.researchr.org","","2022-04-26T06:56:54Z","Code embedding is a keystone in the application of machine learning on several Software Engineering (SE) tasks. To effectively support a plethora of SE tasks, the embedding needs to capture program syntax and semantics in a way that is generic. To this end, we propose the first self-supervised pre-training approach (called GraphCode2Vec) which produces task-agnostic embedding of lexical and program dependence features. GraphCode2Vec achieves this via a synergistic combination of code analysis and Graph Neural Networks. GraphCode2Vec is generic, it allows pre-training, and it is applicable to several SE downstream tasks. We evaluate the effectiveness of GraphCode2Vec on four (4) tasks (method name prediction, solution classification, mutation testing and overfitted patch classification), and compare it with four (4) similarly generic code embedding baselines (Code2Seq, Code2Vec, CodeBERT, GraphCodeBERT) and 7 task-specific, learning-based methods. In particular, GraphCode2Vec is more effective than both generic and task-specific learning-based baselines. It is also complementary and comparable to GraphCodeBERT (a larger and more complex model). We also demonstrate through a probing and ablation study that GraphCode2Vec learns lexical and program dependence features and that self-supervised pre-training improves effectiveness.","2022-05-14T09:36:49Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses - Wei Ma, Mengjie Zhao, Ezekiel Soremekun, Qiang Hu, Jie M. Zhang, Mike Papadakis, Maxime Cordy, Xiaofei Xie, Yves Le Traon",""
"2022-05-19T15:39:00Z","2022-05-19T15:50:00Z","2024-03-21T14:46:32Z","","e91cf013-413e-4a7b-b698-21fb65cbf806@conf.researchr.org","","2022-05-04T03:40:22Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-20T01:00:00Z","2022-05-20T01:07:09Z","2024-03-21T14:46:32Z","","77942db4-db41-4ac9-b6f2-19a2d467e71b@conf.researchr.org","","2022-04-26T06:59:58Z","Many studies have developed Machine Learning (ML) approaches to detect Software Vulnerabilities (SVs) in functions and fine-grained code statements that cause such SVs. However, there is little work on leveraging such detection outputs for data-driven SV assessment to give information about exploitability, impact, and severity of SVs. The information is important to understand SVs and prioritize their fixing. Using large-scale data from 1,782 functions of 429 SVs in 200 real-world projects, we investigate ML models for automating function-level SV assessment tasks, i.e., predicting seven Common Vulnerability Scoring System (CVSS) metrics. We particularly study the value and use of vulnerable statements as inputs for developing the assessment models because SVs in functions are originated in these statements. We show that vulnerable statements are 5.8 times smaller in size, yet exhibit 7.5-114.5% stronger assessment performance (Matthews Correlation Coefficient (MCC)) than non-vulnerable statements. Incorporating context of vulnerable statements further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score). Overall, we provide the initial yet promising ML-based baselines for function-level SV assessment, paving the way for further research in this direction.","2022-05-04T03:43:48Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models - Triet Le Huynh Minh, Muhammad Ali Babar",""
"2022-05-20T01:07:09Z","2022-05-20T01:14:17Z","2024-03-21T14:46:32Z","","329c23b0-1c0c-4a02-8119-5b262345b43b@conf.researchr.org","","2022-04-26T06:59:58Z","Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development workflow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experiments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.","2022-05-04T03:43:48Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] LineVD: Statement-level Vulnerability Detection using Graph Neural Networks - David Hin, Andrey Kan, Huaming Chen, Muhammad Ali Babar",""
"2022-05-20T01:14:17Z","2022-05-20T01:21:26Z","2024-03-21T14:46:32Z","","1403c955-eb94-4950-aec5-5c24cffaa8b3@conf.researchr.org","","2022-04-26T06:59:58Z","Software vulnerabilities are prevalent in software systems, causing a variety of problems including deadlock, information loss, or system failures. Thus, early predictions of software vulnerabilities are critically important in safety-critical software systems. Various ML/DL-based approaches have been proposed to predict vulnerabilities at the file/function/method level. Recently, IVDetect (a graph-based neural network) is proposed to predict vulnerabilities at the function level. Yet, the IVDetect approach is still inaccurate and coarse-grained. In this paper, we propose LineVul, a Transformer-based line-level vulnerability prediction approach in order to address several limitations of the state-of-the-art IVDetect approach. Through an empirical evaluation of a large-scale real-world dataset with 188k+ C/C++ functions, we show that LineVul achieves (1) 160%-379% higher F1-measure for function-level predictions; (2) 12%-25% higher Top-10 Accuracy for line-level predictions; and (3) 29%-53% less Effort@20%Recall than the state-of-the-art approaches. The substantial improvement of our approach highlights the significant contributions towards more accurate, more cost-effective, and more finer-grained software vulnerability predictions.","2022-05-04T03:43:48Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] LineVul: A Transformer-based Line-Level Vulnerability Prediction - Michael Fu, Kla Tantithamthavorn",""
"2022-05-20T01:21:00Z","2022-05-20T01:25:00Z","2024-03-21T14:46:32Z","","66de6477-0063-4ded-b5b7-6f2563375d8f@conf.researchr.org","","2022-04-26T06:59:58Z","Ethereum is the most popular blockchain network because of the introduction of a smart contract. While Ethereum-based software has significantly increased in the wake of their popularity, the carbon emitted by them is pointed to as a global issue.To reduce the emission of carbon, it is necessary to reduce the energy consumed by the software. Recently, most studies have focused on the smart contract and then proposed energy efficiency methods for carbon friendly Ethereum networks. However, it is required to review not only smart contract but also client software for energy used in Ethereum network. This is because the client software performs all functions occurring in Ethereum network that includes smart contracts. Therefore, we need to investigate energy bugs that waste energy in the Ethereum client software, and then study to solve these bugs. The first task to make these studies possible is to build the energy bug benchmark of Ethereum client software. This paper introduces ECench, an energy bug benchmark of Ethereum client software. ECench includes 507 energy buggy commits from 7 series of client software that are nomally operated in Ethereum network. We carefully collected and manually reviewed them for the more clean commits. Another key strength of our benchmark is to provide the 8 categories which cause energy wastage. These categories can serve as a cornerstone for researchers to identify energy waste codes. Consequently, it can provide a valuable starting point for studies for energy reduction, further carbon reduction, in Ethereum.","2022-05-04T03:44:48Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] ECench: An Energy Bug Benchmark of Ethereum Client Software - Jinyoung Kim, Misoo Kim, Eunseok Lee",""
"2022-05-20T01:25:00Z","2022-05-20T01:32:00Z","2024-03-21T14:46:32Z","","f7bd4ce9-cc9b-4494-9c34-406b1c299028@conf.researchr.org","","2022-04-26T06:59:58Z","As any other US software maker, Microsoft is bound by the “Executive Order on Improving the Nation’s Cybersecurity” which dictates a clear mandate to “enhance the software supply chain security” and to generally improve the cyber security practices. To fulfill the executive order, software companies need to enforce new policies and practices on many projects and engineering teams within relatively short periods of time. One challenge is to build up comprehensive inventories of software artifacts which can be tedious and fragile as software eco-systems change rapidly. Required is a system that will constantly monitor and update the inventory of software artifacts and contributors so that at any given point of time. The front line of this security battle includes the product team around the data mining platform CloudMine1 providing the security and compliance teams with engineering artifacts and insights into artifact dependencies and engineering practices of the corresponding engineering teams.","2022-05-04T04:07:26Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Microsoft CloudMine: Data Mining for the Executive Order on Improving the Nation’s Cybersecurity - Kim Herzig, Luke Gostling, Maximilian Grothusmann, Nora Huang, Sascha Just, Alan Klimowski, Yashasvini Ramkumar, Myles McLeroy, Kıvanç Muşlu, Hitesh Sajnani, Varsha Vadaga",""
"2022-05-20T01:32:00Z","2022-05-20T01:36:00Z","2024-03-21T14:46:32Z","","714a750b-67f8-43e2-9ee8-474db7a3f934@conf.researchr.org","","2022-04-26T06:59:58Z","Context: Code Clone Detection (CCD) is a software engineering task that is used for plagiarism detection, code search, and code comprehension. Recently, deep learning-based models have achieved an F1 score (a metric used to assess classifiers) of $\\sim$95% on the CodeXGLUE benchmark. These models require many training data, mainly fine-tuned on Java or C++ datasets. However, no previous study evaluates the generalizability of these models where a limited amount of annotated data is available. \nObjective: The main objective of this research is to assess the ability of the CCD models as well as few shot learning algorithms for unseen programming problems and new languages (i.e., the model is not trained on these problems/languages). \n\\textit{Method:} We assess the generalizability of the state of the art models for CCD in few shot settings (i.e., only a few samples are available for fine-tuning) by setting three scenarios: i) unseen problems, ii) unseen languages, iii) combination of new languages and new problems. We choose three datasets of BigCloneBench, POJ-104, and CodeNet and Java, C++, and Ruby languages. Then, we employ Model Agnostic Meta-learning (MAML), where the model learns a meta-learner capable of extracting transferable knowledge from the train set; so that the model can be fine-tuned using a few samples. Finally, we combine contrastive learning with MAML to further study whether it can improve the results of MAML.","2022-05-04T04:07:27Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Evaluating few shot and Contrastive learning Methods for Code Clone Detection - Mohamad Khajezade, Fatemeh Hendijani Fard, Mohamed S Shehata",""
"2022-05-20T01:36:00Z","2022-05-20T01:50:00Z","2024-03-21T14:46:32Z","","8360f086-a75e-46a8-b4ca-edee7ef0b0fb@conf.researchr.org","","2022-05-04T03:43:48Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-19T16:00:00Z","2022-05-19T16:10:00Z","2024-03-21T14:46:32Z","","0f33a015-4532-46a7-94f8-7f10db9bc9f1@conf.researchr.org","","2022-05-04T06:12:27Z","","2022-05-13T04:28:47Z","MSR Plenary room - , , ","","","[MSR Technical Papers] Back to the future: Empirical Revolution(s) in Software Engineering - Audris Mockus",""
"2022-05-19T16:10:00Z","2022-05-19T16:20:00Z","2024-03-21T14:46:32Z","","c377bc06-f774-48e4-8137-e5c00ae9a72f@conf.researchr.org","","2022-05-04T06:12:27Z","","2022-05-13T04:29:47Z","MSR Plenary room - , , ","","","[MSR Technical Papers] Engineering the MSR Field and the Joy of Research - Ahmed E. Hassan",""
"2022-05-19T16:20:00Z","2022-05-19T16:30:00Z","2024-03-21T14:46:32Z","","6838b6f1-4ebf-4705-9e0f-3d99a64b3a74@conf.researchr.org","","2022-05-04T06:12:27Z","","2022-05-13T04:31:18Z","MSR Plenary room - , , ","","","[MSR Technical Papers] It's all in your network: How mining developer collaboration allowed us to peer into complex socio-technical aspects of software development - Daniela Damian",""
"2022-05-19T16:30:00Z","2022-05-19T16:51:00Z","2024-03-21T14:46:32Z","","8595b896-268f-49e9-8e98-c3dde8ff5d2c@conf.researchr.org","","2022-05-11T15:42:32Z","","2022-05-12T21:21:13Z","MSR Plenary room - , , ","","","[MSR Technical Papers] Discussion",""
"2022-05-19T17:00:00Z","2022-05-19T17:10:00Z","2024-03-21T14:46:32Z","","419b94f6-18d4-45cb-bd4a-33341dd714dd@conf.researchr.org","","2022-05-04T06:16:41Z","","2022-05-13T05:15:23Z","MSR Plenary room - , , ","","","[MSR Technical Papers] Bias in MSR research - Alexander Serebrenik",""
"2022-05-19T17:10:00Z","2022-05-19T17:20:00Z","2024-03-21T14:46:32Z","","cd1540dd-039d-4dd0-b4e1-fc24f2b4d38a@conf.researchr.org","","2022-05-04T06:16:41Z","","2022-05-13T04:28:59Z","MSR Plenary room - , , ","","","[MSR Technical Papers] The Next Generation of Software Developers - Denae Ford",""
"2022-05-19T17:20:00Z","2022-05-19T17:30:00Z","2024-03-21T14:46:32Z","","32684344-3ff2-4842-bb3f-473eba00ab59@conf.researchr.org","","2022-05-04T06:16:41Z","","2022-05-13T04:29:13Z","MSR Plenary room - , , ","","","[MSR Technical Papers] Mining Software Repositories in the age of AI - Foutse Khomh",""
"2022-05-19T17:30:00Z","2022-05-19T17:51:00Z","2024-03-21T14:46:32Z","","b84c0842-161a-4195-bc17-0479a9294d34@conf.researchr.org","","2022-05-11T15:43:25Z","","2022-05-12T21:21:13Z","MSR Plenary room - , , ","","","[MSR Technical Papers] Discussion",""
"2022-05-19T18:00:00Z","2022-05-19T18:50:00Z","2024-03-21T14:46:32Z","","7e0fe631-1e13-4a09-8baf-8d5c85344c53@conf.researchr.org","","2022-05-19T02:08:57Z","","2022-05-19T02:41:19Z","MSR Plenary room - , , ","","","[MSR Technical Papers] MIP Award Talk - Georgios Gousios, Diomidis Spinellis",""
"2022-05-20T02:00:00Z","2022-05-20T02:50:00Z","2024-03-21T14:46:32Z","","68116a19-f7f9-404f-95fb-c55bfa3e5f94@conf.researchr.org","","2022-05-05T14:55:30Z","","2022-05-12T21:21:13Z","MSR Plenary room - , , ","","","[MSR Technical Papers] MSR Foundational Contribution Award - Dongmei Zhang, Tao Xie",""
"2022-05-20T08:00:00Z","2022-05-20T08:04:00Z","2024-03-21T14:46:32Z","","2a485235-46f6-4af2-8f35-781853a79964@conf.researchr.org","","2022-04-26T07:00:51Z","Interpretation has been considered as one of key factors for applying defect prediction in practice. As one way for interpretation, local explanation methods has been widely used for certain predictions on datasets of traditional features. There are also attempts to use local explanation methods on source code-based defect prediction models, but unfortunately, it will get poor results. Since it is unclear how effective those local explanation methods are, we evaluate such methods with automatic metrics which focus on local faithfulness and explanation precision. Based on the results of experiments, we find that the effectiveness of local explanation methods depends on the adopted defect prediction models. They are effective on Bag of Word-based models, while they may not be effective enough to explain all predictions of deep semantic models. Besides, we also find that the hyperparameter of local explanation methods should be carefully optimized to get more precise and meaningful explanation","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Evaluating the effectiveness of local explanation methods on source code-based defect prediction models - Yuxiang Gao, Yi Zhu, Qiao YU",""
"2022-05-20T08:04:00Z","2022-05-20T08:11:00Z","2024-03-21T14:46:32Z","","caa5b575-3a9d-44d6-97a3-24f4f8797f48@conf.researchr.org","","2022-04-26T07:00:51Z","Continuous integration and delivery (CI/CD) has been shown to be very useful to improve the quality of software products (e.g., increasing their reliability or maintainability), and their development processes, e.g., by shortening release cycles. Applying CI/CD in the context of Cyber-Physical Systems (CPSs) can be particularly important, given that many of those systems can have safety-critical properties, and given their interaction with hardware or simulators during the development phase. This paper empirically analyzes how CI/CD is enacted in CPSs when considering the context of open-source projects, that often (also) rely on hosted CI/CD solutions, and benefit of an open-source development community. We qualitatively analyze a statistically significant sample of 670 pull requests from 20 open-source CPSs hosted on GitHub, to identify and categorize—also keeping into account catalogs from previous literature—bad practices, challenges, mitigation and restructuring actions. The study reports and discusses the relationships we found between bad practices/challenges and CI/CD restructuring/mitigation strategies, reporting concrete examples, especially those emerging from the intrinsic complexity of CPSs.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Problems and Solutions in Applying Continuous Integration and Delivery to 20 Open-Source Cyber-Physical Systems - Fiorella Zampetti, Vittoria Nardone, Massimiliano Di Penta",""
"2022-05-20T08:11:00Z","2022-05-20T08:18:00Z","2024-03-21T14:46:32Z","","f5bd3442-4e13-4735-b25b-76519d73f5cc@conf.researchr.org","","2022-04-26T07:00:51Z","JavaScript (JS) is one of the most popular programming languages, and widely used for web apps, mobile apps, desktop clients, and even backend development. Due to its dynamic and flexible nature, however, JS applications often have a reputation for poor software quality. As a type-safe superset of JavaScript, TypeScript (TS) offers features to address these prejudices. However, there is currently insufficient empirical evidence to broadly support the claim that TS applications exhibit better software quality than JS applications. \nWe therefore conducted a repository mining study based on 604 GitHub projects (299 for JS, 305 for TS) with over 16M LoC. Using SonarQube and the GitHub API, we collected and analyzed four facets of software quality: a) code quality (# of code smells per LoC), b) code understandability (cognitive complexity per LoC), c) bug proneness (bug fix commit ratio), and d) bug resolution time (mean time a bug issue is open). For TS, we also collected how frequently the type-safety ignoring any type was used per project via ESLint. \nThe analysis indicates that TS applications exhibit significantly better code quality and understandability than JS applications. Contrary to expectations, however, bug proneness and bug resolution time of our TS sample were not significantly lower than for JS: the mean bug fix commit ratio of TS projects was more than 60% larger (0.126 vs. 0.206), and TS projects needed on average more than an additional day to fix bugs (31.86 vs. 33.04 days). Furthermore, reducing the usage of the any type in TS apps appears to be beneficial: its frequency was significantly correlated with all metrics except bug proneness, even though the correlations were of small strengths (Spearman’s rho between 0.17 and 0.26). \nOur results indicate that the perceived positive influence of TypeScript for avoiding bugs in comparison to JavaScript may be more complicated than assumed. While using TS seems to have benefits, it does not automatically lead to less and easier to fix bugs. However, more research is needed in this area, especially concerning the potential influence of project complexity and developer experience.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] To Type or Not to Type? A Systematic Comparison of the Software Quality of JavaScript and TypeScript Applications on GitHub - Justus Bogner, Manuel Merkel",""
"2022-05-20T08:18:00Z","2022-05-20T08:25:00Z","2024-03-21T14:46:32Z","","803b0de0-99e8-4562-b697-46fde967d772@conf.researchr.org","","2022-04-26T07:00:51Z","Background: Selecting a suitable feature reduction technique, when building a defect prediction model, can be challenging. Different techniques can result in the selection of different independent variables which have an impact on the overall performance of the prediction model. To help in the selection, previous studies have assessed the impact of each feature reduction technique using different datasets. However, there are many reduction techniques, and therefore some of the well-known techniques have not been assessed by those studies. Aim: The goal of the study is to select a high-accuracy reduction technique from several candidates without preliminary assessments. Method: We utilized bandit algorithm (BA) to help with the selection of best features reduction technique for a list of candidates. To select the best feature reduction technique, BA evaluates the prediction accuracy of the candidates, comparing testing results of different modules with their prediction results. By substituting the reduction technique for the prediction method, BA can then be used to select the best reduction technique. Results: In the experiment, we evaluated the performance of BA to select suitable reduction technique. We performed cross version defect prediction using 14 datasets. As feature reduction techniques, we used two assessed and two non-assessed techniques. Using BA, the prediction accuracy was higher or equivalent than existing approaches on average, compared with techniques selected based on an assessment. Conclusions: BA can have larger impact on improving prediction models by helping not only on selecting suitable models, but also in selecting suitable feature reduction techniques.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Using Bandit Algorithms for Selecting Feature Reduction Techniques in Software Defect Prediction - Masateru Tsunoda, Akito Monden, Koji Toda, Amjed Tahir, Kwabena Ebo Bennin, Keitaro Nakasai, Masataka Nagura, Kenichi Matsumoto",""
"2022-05-20T08:25:00Z","2022-05-20T08:29:00Z","2024-03-21T14:46:32Z","","fd851150-2dab-4dac-92d8-e07905ec7adf@conf.researchr.org","","2022-04-26T07:00:51Z","Since programming languages offer a wide variety of grammers, desired functions can be implemented in a variety of ways. We consider that there is a large amount of source code that has different implementations of the same functions, and that those can be compiled into a dataset that can be used for various research in software engineering. In this study, we construct a dataset of Java methods with functionally equivalent functions from about 36 million lines of source code. The constructed dataset is available at https://zenodo.org/record/5896268.","2022-05-26T07:45:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Constructing Dataset of Functionally Equivalent Java Methods Using Automated Test Generation Techniques - Yoshiki Higo, Shinsuke Matsumoto, Shinji Kusumoto, Kazuya Yasuda",""
"2022-05-20T08:29:00Z","2022-05-20T08:36:00Z","2024-03-21T14:46:32Z","","4df94e8c-0085-4c38-b66a-ec785b371d8a@conf.researchr.org","","2022-04-26T07:00:51Z","This industrial problem aims to determine and structure, in a suitable taxonomy, the most common anomalies and related corrective actions present in software development starting from information stored in code repositories.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Extracting corrective actions from code repositories - Yegor Bugayenko, Kirill Daniakin, Mirko Farina, Firas Jolha, Artem Kruglov, Witold Pedrycz, Giancarlo Succi",""
"2022-05-20T08:36:00Z","2022-05-20T08:50:00Z","2024-03-21T14:46:32Z","","977b3045-c82c-429f-b2e6-76e85ca3e5a6@conf.researchr.org","","2022-05-04T03:45:37Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-20T18:00:00Z","2022-05-20T18:07:30Z","2024-03-21T14:46:32Z","","96f64564-fe9f-4c41-ba4f-d3b0351f924f@conf.researchr.org","","2022-04-26T07:04:56Z","Nowadays, websites commonly run web applications on the server side to handle HTTP requests and generate responses dynamically. These server-side web applications handle a large number of concurrent requests and are thus highly vulnerable to request races, i.e., races while handling concurrent requests. To better handle such request races in server-side web applications, we need a deep understanding of their characteristics. While some previous studies of real-world request races exist, they primarily focus on the root cause of these bugs. In this paper, we provide a complementary focus on their effects and fixes. We study the external and internal effects of request races, and we relate request-race fixes with concurrency control mechanisms in languages and frameworks for developing server-side web applications. Our study reveals several interesting findings, and we expect our results can help developers better understand request races and guide the design and development of tools for combating request races.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] A Deep Study of the Effects and Fixes of Server-Side Request Races in Web Applications - Zhengyi Qiu, Shudi Shao, Qi Zhao, Hassan Ali Khan, Xinning Hui, Guoliang Jin",""
"2022-05-20T18:07:00Z","2022-05-20T18:11:00Z","2024-03-21T14:46:32Z","","dee9e158-79e0-4dbf-b47e-3c2408adb887@conf.researchr.org","","2022-04-26T07:04:56Z","We introduce a large-scale dataset of the complete texts of free/open source software (FOSS) license variants. To assemble it we have collected from the Software Heritage archive—the largest publicly available archive of FOSS source code with accompanying development history—all versions of files whose names are commonly used to convey licensing terms to software users and developers. The dataset consists of 6.5 million unique license files that can be used to conduct empirical studies on open source licensing, training of automated license classifiers, natural language processing (NLP) analyses of legal texts, as well as historical and phylogenetic studies on FOSS licensing. Additional metadata about shipped license files are also provided, making the dataset ready to use in various contexts; they include: file length measures, detected MIME type, detected SPDX license (using ScanCode), example origin (e.g., GitHub repository), oldest public commit in which the license appeared. The dataset is released as open data as an archive file containing all deduplicated license blobs, plus several portable CSV files for metadata, referencing blobs via cryptographic checksums.","2022-05-14T08:53:41Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] A Large-scale Dataset of (Open Source) License Text Variants - Stefano Zacchiroli",""
"2022-05-20T18:11:00Z","2022-05-20T18:18:00Z","2024-03-21T14:46:32Z","","2c32c4c4-daf4-49bf-83be-656eab027daa@conf.researchr.org","","2022-04-26T07:04:56Z","Context. Detecting and especially assessing software vulnerabilities continues to be a challenge in the vulnerability prediction field mainly due to the poor quality and/or low amount of data curated [1]. Many works were conducted aiming to create datasets of security patches based on software repositories data [2,3,4,5]. However, there are still very few known gold standard datasets for comparison/evaluation of the different approaches [6]. One way to detect/assess software vulnerabilities is by extracting security-related information from commit messages. Yet, automating the detection and assessment of vulnerabilities upon security commit messages is still challenging due to the lack of structured and clear messages. \nAre security-relevant commit messages informative? We conducted an empirical analysis of 2k security commit messages collected from GitHub commits included in CVE reports references; and, confirmed that 23% of the commit messages used to patch publicly known vulnerabilities are either 1) cryptic/poorly documented, or 2) do not seem security-related (unclear). Results suggest that best practices/templates are necessary to help security engineers create better security commit messages; and further technology development upon this type of repository data, i.e., commits messages. \nHow to write a good security commit messages? We searched for conventions or guidelines on writing security commits messages. But we only found guidelines to write better generic commit messages which do not consider crucial security-related information such as the CWE-ID, CVE-ID, impact/score of the vulnerability, and more. These bits of security-related information are essential in detecting and assessing vulnerabilities through commit messages for both humans and tools. Therefore, we created a convention for security commit messages that structure and contemplate information about the vulnerabilities. \nSECOM: A convention for security commit messages. This convention was created upon well-known sources on writing better commits messages—provided at the end of our website—to facilitate its adoption. The structure and set of fields included in the convention were inferred 1) from the conclusions retained from our empirical analysis of security-related commits messages; and, 2) from feedback collected by presenting SECOM in two Open Source Security Foundation working groups. The full convention, details, and examples are available here: https://tqrg.github.io/secom/. \nFeedback and Future Ideas: In general, the community sees value in SECOM and would like to see it as a standard practice. We are currently working with the Open Source Vulnerability Database Google team to gather internal feedback from their teams. Writing more structured and informative commit messages for vulnerability disclosure/patching will further the detection and assessment of security vulnerabilities through commit messages. In the future, new technologies can be developed on top of SECOM to boost team productivity with tools to assess compliance or to assist developers in writing better commit messages with recommendations and auto-completion.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] SECOM: Towards a convention for security commit messages - Sofia Reis, Rui Abreu, Hakan Erdogmus, Corina S. Păsăreanu",""
"2022-05-20T18:18:00Z","2022-05-20T18:25:00Z","2024-03-21T14:46:32Z","","9bd17c19-de36-4cfd-bec8-e63da821a5b6@conf.researchr.org","","2022-04-26T07:04:56Z","The complexity and scale of modern software programs often lead to overlooked programming errors and security vulnerabilities. Developers often rely on automatic tools, like static analysis tools, to look for bugs and vulnerabilities. Static analysis tools are widely used because they can understand nontrivial program behaviors, scale to millions of lines of code, and detect subtle bugs. However, they are known to generate an excess of false alarms which hinder their utilization as it is counterproductive for developers to go through a long list of reported issues, only to find a few true positives. One of the ways proposed to suppress false positives is to use machine learning to identify them. However, training machine learning models requires good quality labeled datasets. For this purpose, we developed D2A , a differential analysis based approach that uses the commit history of a code repository to create a labeled dataset of Infer static analysis output. \nThe data generated by D2A can be used to train AI Models like Voting, Stacking Ensembles and C-BERT , which learn to identify False Positives. Ensembles are built on top of the Boosting and Tree based classifiers which use hand-crafted features for classifying static analyzer output as True Positives. C-BERT is a BERT-base language model pretrained from scratch on C source code extracted from 10,000 C repositories, thus leveraging “big code”. This model is then fine-tuned on D2A labeled data. This approach views source code as language and automates the extraction of features. The output of the models is a prioritized list of defects ordered by the likelihood of being True Positive. \nOne way to use the Augmented Static Analyzer to improve developer productivity would be to insert the application in the developer workflow so that its use would seem natural. With this idea in mind, we created Varangian, which is a Git bot that automatically creates issues on the repository based on defects prioritized by the Augmented Static Analyzer. Models trained on the D2A dataset created from the same repository are used to create the prioritized list. \nThe inference pipeline which produces the prioritized list first applies Infer static analyzer on the latest commit of the repository. Relevant source code is then extracted based on bug report for each defect highlighted by the static analyzer. Bug reports and relevant source code extracted from repositories are used to generate features for AI Models, which then assign a likelihood of being a True Positive to each defect. The git bot then takes the defects most likely to be True Positive and creates issues for each defect. The issue created by Varangian has a lot of information the developer can use for debugging. \nThe most recent run of Varangian bot prototype on the latest commit of an opensource project created 5 issues out of which 1 was a TP. This gives us an FP/TP ratio of 4/1 which is five times better than the 20/1 ratio we observe for Infer on the test set of the same project. In this presentation, we will showcase Varangian, compare different model training approaches, discuss the challenges involved in building a training and inference pipeline based on code repository, and the impact on performance when moving from the test set to the latest commit.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Varangian: A Git Bot for Augmented Static Analysis - Saurabh Pujar, Yunhui Zheng, Luca Buratti, Burn Lewis, Alessandro Morari, Jim A. Laredo, Kevin Postlethwait, Christoph Görn",""
"2022-05-20T18:25:00Z","2022-05-20T18:32:00Z","2024-03-21T14:46:32Z","","b09dd810-1772-4eb9-85d5-118af31ee6bc@conf.researchr.org","","2022-04-26T07:04:56Z","","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Detecting Privacy-Sensitive Code Changes with Language Modeling - Gökalp Demirci, Vijayaraghavan Murali, Imad Ahmad, Rajeev Rao, Gareth Ari Aye",""
"2022-05-20T18:32:00Z","2022-05-20T18:36:00Z","2024-03-21T14:46:32Z","","9f4209b0-2bcb-4c54-9470-c0f95524c4ea@conf.researchr.org","","2022-04-26T07:04:56Z","\\textit{Background/Context:} Several advances in deep learning have been successfully applied to the software development process. Of recent interest is the use of neural language models to build tools that can assist in writing code. There is a growing body of work to evaluate these tools and their underlying language models. We aim to contribute to this line of research via a comparative empirical analysis of these tools and language models from a security perspective. For the rest of this paper, we use CGT (Code Generation Tool) to refer to language models as well as other tools, such as Copilot, that are built with language models. \n\\textit{Objective/Aim:} The aim of this study is to compare the performance of CGTs and human developers. Specifically, we investigate whether CGTs are just as likely to introduce the same software vulnerabilities as human developers. \n\\textit{Method:} We will use the Big-Vul dataset proposed by Fan et al.~\\cite{fan_cc_2020} - a dataset of vulnerabilities introduced by human developers. For each entry in the dataset, we will recreate the scenario before the bug was introduced and allow the CGT to generate a completion. The completions are manually inspected in order to be classified as 1. containing the same vulnerability (introduced by the human), 2. containing a fix for the vulnerability or 3. other. The \\emph{other} category is used as a catchall for scenarios that are out of scope for this project.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code? - Owura Asare, Mei Nagappan, N. Asokan",""
"2022-05-20T18:36:00Z","2022-05-20T18:43:00Z","2024-03-21T14:46:32Z","","cc536338-12c0-4338-ba46-899ba33e4257@conf.researchr.org","","2022-05-07T10:24:10Z","GitHub’s mission is to accelerate human progress through developer collaboration. To better understand how well we’re achieving this mission, it’s crucial to investigate the off-platform effects of on-platform activity. In this lightning talk, I will present my explorations of the social dynamics of a handful of venture-backed open-core companies using GitHub data, with the hope of inspiring future research collaborations on questions of the societal impact of open source.","2022-05-12T20:41:58Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Finding the Fun in Fundraising: Public Issues and Pull Requests in VC-backed Open-Core Companies - Kevin Xu",""
"2022-05-20T18:43:00Z","2022-05-20T19:00:00Z","2024-03-21T14:46:32Z","","6a537351-2fa3-454a-80d8-99c207d288b3@conf.researchr.org","","2022-05-08T11:38:04Z","","2022-05-14T09:49:00Z","MSR Main room - even hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-20T15:00:00Z","2022-05-20T15:07:00Z","2024-03-21T14:46:32Z","","dbc7a229-962b-44c6-be35-acbc20fb8d7f@conf.researchr.org","","2022-04-26T07:02:19Z","Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the codebase. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria in the context of MCR. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Code Review Practices for Refactoring Changes: An Empirical Study on OpenStack - Eman Abdullah AlOmar, Moataz Chouchen, Mohamed Wiem Mkaouer, Ali Ouni",""
"2022-05-20T15:07:00Z","2022-05-20T15:11:00Z","2024-03-21T14:46:32Z","","bbfac0da-b370-44fe-a238-d2cf555b20ac@conf.researchr.org","","2022-04-26T07:02:19Z","The field of Automated Program Repair (APR) has received increasing attention in recent years both from the academic world and from leading IT companies. It’s main goal is to repair software bugs automatically, thus reducing the cost of development and maintenance significantly. Recent works use state-of-the-art deep learning models to predict correct patches, for these teaching on a large amount of data is inevitable almost in every scenarios. Despite this, readily accessible data on the field is very scarce. To contribute to related research, we present FixJS, a dataset containing bug-fixing information of ~ 2 million commits. The commits were gathered from GitHub and processed locally to have both the buggy (before bug fixing commit) and fixed (after fix) version of the same program. We focused on JavaScript functions, as it is one of the most popular programming language globally and functions are first class objects there. The data includes more than 300.000 samples of such functions, including commit information, before/after states and 3 source code representations.","2022-05-11T09:00:09Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] FixJS: A Dataset of Bug-fixing JavaScript Commits - Viktor Csuvik, László Vidács",""
"2022-05-20T15:11:00Z","2022-05-20T15:15:00Z","2024-03-21T14:46:32Z","","23debf27-2b2f-4ca3-874e-e0af80d504a1@conf.researchr.org","","2022-04-26T07:02:19Z","Software evolution is the process of developing, maintaining, and updating software systems. It is known that the software systems tend to increase their complexity and size over their evolution to meet the demands required by the users. Due to this fact, researchers have increasingly carried out studies on software evolution to understand the systems’ evolution pattern and propose techniques to overcome inherent problems in software evolution. Many of these works collect data but do not make them publicly available. Many datasets on software evolution are outdated, and/or are small, and some of them do not provide time series from software metrics. We propose an extensive software evolution dataset with temporal information about open-source Java systems. To build this dataset, we proposed a methodology of four steps: selecting the systems using a criterion, extracting and measuring their releases, and generating their time series. Our dataset contains time series of 46 software metrics extracted from 46 open-source Java systems, and we make it publicly available.","2022-05-28T03:52:26Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] A Time Series-Based Dataset of Open-Source Software Evolution - Bruno L. Sousa, Mariza Bigonha, Kecia A. M. Ferreira, Glaura C. Franco",""
"2022-05-20T15:15:00Z","2022-05-20T15:19:00Z","2024-03-21T14:46:32Z","","c0b40910-e397-4036-b507-fe5255273fb0@conf.researchr.org","","2022-04-26T07:02:19Z","This paper presents LAGOON – an open source platform for understanding the complex ecosystems of Open Source Software (OSS) communities. The platform currently utilizes spatiotemporal graphs to store and investigate the artifacts produced by these communities, and help analysts identify bad actors who might compromise an OSS project’s security. LAGOON provides ingest of artifacts from several common sources, including source code repositories, issue trackers, mailing lists and scraping content from project websites. Ingestion utilizes a modular architecture, which supports incremental updates from data sources and provides a generic identity fusion process that can recognize the same community members across disparate accounts. A user interface is provided for visualization and exploration of an OSS project’s complete sociotechnical graph. Scripts are provided for applying machine learning to identify patterns within the data. While current focus is on the identification of bad actors in the Python community, the platform’s reusability makes it easily extensible with new data and analyses, paving the way for LAGOON to become a comprehensive means of assessing various OSS-based projects and their communities.","2022-05-14T14:17:12Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] LAGOON: An Analysis Tool for Open Source Communities - Sourya Dey, Walt Woods",""
"2022-05-20T15:19:00Z","2022-05-20T15:23:00Z","2024-03-21T14:46:32Z","","b41be829-e870-4506-bdd2-3863d9afe1dc@conf.researchr.org","","2022-04-26T07:02:19Z","Agile software development is nowadays a widely adopted practise in both open-source and industrial software projects. Agile teams typically heavily rely on issue management tools to document new issues and keep track of outstanding ones, in addition to storing their technical details, effort estimates, assignment to developers, and more. Previous work utilised the historical information stored in issue management systems for various purposes; however, when researchers make their empirical data public, it is usually relevant solely to the study’s objective. In this paper, we present a more holistic and versatile dataset containing a wealth of information on more than 480,000 issues from 44 open-source Agile software, making it well-suited to several research avenues, and cross-analyses therein, including effort estimation, issue prioritization, issue assignment and many more. We make this data publicly available on GitHub to facilitate ease of use, maintenance, and extensibility.","2022-05-05T12:20:49Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] A Versatile Dataset of Agile Open Source Software Projects - Vali Tawosi, Afnan Al-Subaihin, Rebecca Moussa, Federica Sarro",""
"2022-05-20T15:23:00Z","2022-05-20T15:30:00Z","2024-03-21T14:46:32Z","","fa6fca37-a6bf-4831-8e86-57268fd12559@conf.researchr.org","","2022-04-26T07:02:19Z","Automatically prioritizing software development tasks extracted from codes could provide significant technical and organizational advantages. Tools exist for the automatic extraction of tasks, but they still lack the ability to capture their mutual dependencies; hence, the capability to prioritize them.","2022-05-04T04:09:11Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Automatically Prioritizing and Assigning Tasks from Code Repositories in Puzzle Driven Development - Ayomide Bakare, Yegor Bugayenko, Arina Cheverda, Mirko Farina, Artem Kruglov, Witold Pedrycz, Giancarlo Succi",""
"2022-05-20T15:30:00Z","2022-05-20T15:34:00Z","2024-03-21T14:46:32Z","","2282b3be-29bc-4c7a-b237-5eb507179d7d@conf.researchr.org","","2022-04-26T07:02:19Z","The traditional path to a software engineering career involves a post-secondary diploma in Software Engineering, Computer Science, or a related field. However, many software engineers take a non-traditional path to their career, starting from other industries or fields of study. This paper proposes a study on barriers faced by software engineers with non-traditional educational and occupational backgrounds, and possible mitigation strategies for those barriers. We propose a two-stage methodology, consisting of an exploratory study, followed by a validation study. The exploratory study will involve a grounded-theory-based qualitative analysis of relevant Reddit data to yield a framework around the barriers and possible mitigation strategies. These findings will then be validated using a survey in the validation study. Making software engineering more accessible to those with non-traditional backgrounds will not only bring about the benefits of functional diversity, but also serves as a method of filling in the labour shortages of the software engineering industry.","2022-05-04T04:09:10Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Towards Understanding Barriers and Mitigation Strategies of Software Engineers with Non-traditional Educational and Occupational Backgrounds - Tavian Barnes, Ken Jen Lee, Cristina Tavares, Gema Rodríguez-Pérez, Mei Nagappan",""
"2022-05-20T15:34:00Z","2022-05-20T15:38:00Z","2024-03-21T14:46:32Z","","4422f918-d7d7-4262-8ef4-c7d7b8f263c1@conf.researchr.org","","2022-04-26T07:02:19Z","Although architecture instability has been studied and measured using a variety of metrics, a deeper analysis of which project parts are less stable and how such instability varies over time is still needed. While having more information on architecture instability is, in general, useful for any software development project, it is especially important in Open Source Software (OSS) projects where the supervision of the development process is more difficult to achieve. In particular, we are interested when OSS projects grow from a small controlled environment (i.e., the cathedral phase) to a community-driven project (i.e., the bazaar phase). In such a transition, the project often explodes in terms of software size and number of contributing developers. Hence, the complexity of the newly added features, and the frequency of the commits and files modified may cause significant variations of the instability of the structure of the classes and packages. Consequently, in this registered report we suggest ways to analyze the instability in OSS projects, especially during that sensitive phase where they become community-driven. We intend to suggest ways to predict the evolution of the instability in several OSS projects. Our preliminary results show that it seems possible to provide meaningful estimations that can be useful for OSS teams before a project grows in excess.","2022-05-04T04:09:09Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Can instability variations warn developers when open-source projects boost? - Alejandro Valezate, Rafael Capilla, Gregorio Robles, Victor Salamanca",""
"2022-05-20T15:38:00Z","2022-05-20T15:50:00Z","2024-03-21T14:46:32Z","","941bbd7e-d37c-4ac2-8fa4-ec5cfee31296@conf.researchr.org","","2022-05-04T03:47:25Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-20T16:00:00Z","2022-05-20T16:50:00Z","2024-03-21T14:46:32Z","","f765838f-1b7a-4866-af53-a3cd3d686132@conf.researchr.org","","2022-04-26T07:06:10Z","","2022-04-26T07:06:10Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Using Datalore for Reproducible Research - Jodie Burchell",""
"2022-05-20T17:00:00Z","2022-05-20T17:05:00Z","2024-03-21T14:46:32Z","","64d52f8b-1a0f-4107-8587-c67aee2164c0@conf.researchr.org","","2022-04-26T07:03:45Z","Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the Grimoirelab development analytics platform. In this work we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using a Kibana dashboard.","2022-05-04T03:53:47Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Bot Detection in GitHub Repositories - Natarajan Chidambaram, Pooya Rostami Mazrae",""
"2022-05-20T17:05:00Z","2022-05-20T17:10:00Z","2024-03-21T14:46:32Z","","be67328a-49a9-484b-9310-162b89200f75@conf.researchr.org","","2022-04-26T07:03:45Z","Open-source repositories provide wealth of information and are increasingly being used to build artificial intelligence (AI) based systems to solve problems in software engineering. Open-source repositories could be of varying quality levels, and bad-quality repositories could degrade performance of these systems. Evaluating quality of open-source repositories, which is not available directly on code hosting sites such as GitHub, is thus important. In this hackathon, we utilize known code quality measures and GrimoireLab toolkit to implement a framework, named GitRank, to rank open-source repositories on three different criteria. We discuss our findings and preliminary evaluation of GitRank in this hackathon report.","2022-05-06T03:08:52Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] GitRank: A Framework to Rank GitHub Repositories - Niranjan Hasabnis",""
"2022-05-20T17:10:00Z","2022-05-20T17:15:00Z","2024-03-21T14:46:32Z","","d9bbe25a-1ac6-420b-a40c-99a3538180ef@conf.researchr.org","","2022-04-26T07:03:45Z","E-type open-source software inevitably grows in size and complexity over time, and without performing anti-regressive tasks this type of software has a limited lifespan. In this project, a case study of the effect of such anti-regressive tasks is conducted using GrimoireLab Graal as a subject. This process is guided by quality metrics and developer insights. The outcome of this work is a life-cycle of maintenance activities, ultimately resulting in a refactored version of GrimoireLab Graal. After applying anti-regressive actions, commonly used software quality metrics decreased (lower is better). Additionally, after performing an experiment to test the evolution readiness of the software, the complexity of the original software increased significantly, whilst no side effects were measured in the revised software.","2022-05-26T17:20:49Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] GrimoireLab Maintenance and Evolution - Willem Meijer, David Visscher, Erwin de Haan, Merijn Schröder, Leon Visscher, Andrea Capiluppi, Ioan Botez",""
"2022-05-20T17:15:00Z","2022-05-20T17:20:00Z","2024-03-21T14:46:32Z","","540b8c5b-ca55-4148-ac8d-78ab1bd2604e@conf.researchr.org","","2022-04-26T07:03:45Z","Version 3.0.0 of OpenSSL was the first release built with public design documents. We compare code quality of this version with the previous major release using a new backend for Graal that we developed during the Hackathon. We find that while code quality improved for both releases during their development, the improvements were larger for version 3.0.0. However, consistency of coding style decreased faster during development of the current version.","2022-05-18T04:56:27Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] OpenSSL 3.0.0: An exploratory case study - James Walden",""
"2022-05-20T17:20:00Z","2022-05-20T17:25:00Z","2024-03-21T14:46:32Z","","f0453080-96d8-4383-90d5-6d03c7ceb325@conf.researchr.org","","2022-04-26T07:03:45Z","We explore the role of reciprocity in code review processes. Reciprocity manifests itself in two ways: 1) reviewing code for others translates to accepted code contributions, and 2) having contributions accepted increase the reviews made for others. We use vector autoregressive (VAR) models to explore the causal relation between reviews performed and accepted contributions. After fitting VAR models for 24 active open-source developers, we found evidence of reciprocity in 6 of them. These results suggest reciprocity does play a role in code review, that can potentially be exploited to increase reviewer participation.","2022-05-06T01:46:01Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Quid Pro Quo: An Exploration of Reciprocity in Code Review - Carlos Gavidia-Calderon, DongGyun Han, Amel Bennaceur",""
"2022-05-20T17:25:00Z","2022-05-20T17:30:00Z","2024-03-21T14:46:32Z","","8ff0b37b-5643-4aed-a906-d3e3fdb40318@conf.researchr.org","","2022-04-26T07:03:45Z","In this paper, we present our MSR Hackathon 2022 project that replicates an existing Gitter study using GrimoireLab. We compare the previous study’s pipeline with our GrimoireLab implementation in terms of speed, data consistency, organization, and the learning curve to get started. We believe our experience with GrimoireLab can help future researchers in making the right choice while implementing their data pipelines over Gitter and Github data.","2022-05-07T22:40:03Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Replicating Data Pipelines with GrimoireLab - Kalvin Eng, Hareem Sahar",""
"2022-05-20T17:30:00Z","2022-05-20T17:50:00Z","2024-03-21T14:46:32Z","","ef6d9035-8e57-496e-b4ba-478853964725@conf.researchr.org","","2022-05-04T03:52:37Z","","2022-05-14T09:49:00Z","MSR Main room - odd hours - , , ","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-20T17:00:00Z","2022-05-20T17:50:00Z","2024-03-21T14:46:32Z","","ba4008a7-c621-46af-b50a-50a996e1c2d9@conf.researchr.org","","2022-04-26T07:05:56Z","","2022-04-26T07:05:56Z","MSR Tutorials room - , , ","","","[MSR Technical Papers] Software Bots in Software Engineering: Benefits and Challenges - Mairieli Wessel, Marco Gerosa, Emad Shihab",""
"2022-05-23T13:00:00Z","2022-05-23T13:20:00Z","2024-03-21T14:46:32Z","","1bae3277-0291-46c4-8879-48c2edc64b73@conf.researchr.org","","2022-05-17T04:08:46Z","","2022-05-19T04:50:18Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] In-Person MSR 2022 Opening Session - David Lo, Shane McIntosh, Nicole Novielli",""
"2022-05-23T13:20:00Z","2022-05-23T13:55:00Z","2024-03-21T14:46:32Z","","e5240103-8089-4538-9969-2c51690e5576@conf.researchr.org","","2022-05-17T04:08:46Z","","2022-05-19T04:50:32Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] From Models to Systems: Rethinking the Role of Software Engineering for Machine Learning - Christian Kästner",""
"2022-05-23T13:55:00Z","2022-05-23T14:30:00Z","2024-03-21T14:46:32Z","","bf27d0b5-4244-4333-a025-8f54bbe77769@conf.researchr.org","","2022-05-19T02:41:19Z","","2022-05-19T04:50:40Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] MIP Award Talk - Georgios Gousios, Diomidis Spinellis",""
"2022-05-23T15:00:00Z","2022-05-23T15:15:00Z","2024-03-21T14:46:32Z","","55888c13-f4e9-4ce1-9e62-5f13cfd0c5f4@conf.researchr.org","","2022-05-13T04:44:54Z","Code velocity, or the speed with which code changes are integrated into a production environment, plays a crucial role in Continuous Integration and Continuous Deployment. Many studies report factors influencing code velocity. However, solutions to increase code velocity are unclear. Meanwhile, the industry continues to issue guidelines on “ideal” code change size, believing it increases code velocity despite lacking evidence validating the practice. Surprisingly, this fundamental question has not been studied to date. This study investigates the practicality of improving code velocity by optimizing pull request size and composition(ratio of insertions, deletions, and modifications). We start with a hypothesis that a moderate correlation exists between pull request size and time-to-merge. We selected 100 most popular, actively developed projects from 10 programming languages on GitHub. We analyzed our dataset of 845,316 pull requests by size, composition, and context to explore its relationship to time-to-merge—a proxy to measure code velocity. Our study shows that pull request size and composition do not relate to time-to-merge. Regardless of the contextual factors that can influence pull request size or composition (e.g., programming language), the observation holds. Pull request data from two other platforms: Gerrit and Phabricator (401,790 code reviews) confirms the lack of relationship. This negative result as in “… eliminate useless hypotheses … ” [73] challenges a widespread belief by showing that small code changes do not merge faster to increase code velocity.","2022-05-14T08:54:13Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Do Small Code Changes Merge Faster? A Multi-Language Empirical Investigation - Gunnar Kudrjavets, Nachiappan Nagappan, Ayushi Rastogi",""
"2022-05-23T15:15:00Z","2022-05-23T15:30:00Z","2024-03-21T14:46:32Z","","384f4812-8941-46db-8d8c-ddbd2f42286d@conf.researchr.org","","2022-05-13T04:44:54Z","Increasing code velocity (or the speed with which code changes are reviewed and merged) is integral to speeding up development and contributes to the work satisfaction of engineers. While factors affecting code change acceptance have been investigated in the past, solutions to decrease the code review lifetime are less understood. This study investigates the code review process to quantify delays and investigate opportunities to potentially increase code velocity.We study the temporal characteristics of half a million code reviews hosted on Gerrit and Phabricator, starting from the first response, to a decision to accept or reject the changes, and until the changes are merged into a target branch. We identified two types of time delays: (a) the wait time from the proposal of code changes until first response, and (b) the wait time between acceptance and merging. Our study indicates that reducing the time between acceptance and merging has the potential to speed up Phabricator code reviews by 29–63%. Small code changes and changes made by authors with a large number of previously accepted code reviews have a higher chance of being immediately accepted, without code review iterations. Our analysis suggests that switching from manual to automatic merges can help increase code velocity.","2022-05-14T08:54:21Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Mining Code Review Data to Understand Waiting Times Between Acceptance and Merging: An Empirical Analysis - Gunnar Kudrjavets, Aditya Kumar, Nachiappan Nagappan, Ayushi Rastogi",""
"2022-05-23T15:30:00Z","2022-05-23T15:38:00Z","2024-03-21T14:46:32Z","","75f87f78-fe8d-460e-b970-21b118e3844f@conf.researchr.org","","2022-05-13T04:44:54Z","Third party libraries are used to integrate existing solutions for common problems and help speed up development. The use of third party libraries, however, can carry risks, for example through vulnerabilities in these libraries. Studying the dependency networks of package managers lets us better understand and mitigate these risks. So far, the dependency networks of the three most important package managers of the Apple ecosystem, CocoaPods, Carthage and Swift PM, have not been studied. We analysed the dependencies for all publicly available open source libraries up to December 2021 and compiled a dataset containing the dependency networks of all three package managers. The dependency networks can be used to analyse how vulnerabilities are propagated through transitive dependencies. In order to ease the tracing of vulnerable libraries we also queried the NVD database and included publicly reported vulnerabilities for these libraries in the dataset.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Dataset: Dependency Networks of Open Source Libraries Available Through CocoaPods, Carthage and Swift PM - Kristiina Rahkema, Dietmar Pfahl",""
"2022-05-23T15:38:00Z","2022-05-23T15:46:00Z","2024-03-21T14:46:32Z","","6d370492-204b-4f45-ae62-28e42a898099@conf.researchr.org","","2022-05-13T04:44:54Z","We introduce a large-scale dataset of the complete texts of free/open source software (FOSS) license variants. To assemble it we have collected from the Software Heritage archive—the largest publicly available archive of FOSS source code with accompanying development history—all versions of files whose names are commonly used to convey licensing terms to software users and developers. The dataset consists of 6.5 million unique license files that can be used to conduct empirical studies on open source licensing, training of automated license classifiers, natural language processing (NLP) analyses of legal texts, as well as historical and phylogenetic studies on FOSS licensing. Additional metadata about shipped license files are also provided, making the dataset ready to use in various contexts; they include: file length measures, detected MIME type, detected SPDX license (using ScanCode), example origin (e.g., GitHub repository), oldest public commit in which the license appeared. The dataset is released as open data as an archive file containing all deduplicated license blobs, plus several portable CSV files for metadata, referencing blobs via cryptographic checksums.","2022-05-14T08:54:33Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] A Large-scale Dataset of (Open Source) License Text Variants - Stefano Zacchiroli",""
"2022-05-23T15:46:00Z","2022-05-23T15:54:00Z","2024-03-21T14:46:32Z","","a045995e-47ba-4085-8e52-ff0b5ac00fe0@conf.researchr.org","","2022-05-13T04:44:54Z","Single statement bugs are one of the most important ingredients in the evaluation of modern bug detection and automatic program repair methods. By affecting only a single statement, single statement bugs represent a type of bug often overlooked by developers, while still being small enough to be detected and fixed by automatic methods. \nWith the rise of data-driven automatic repair the availability of single statement bugs at the scale of millionth of examples is more important than ever; not only for testing these methods but also for providing sufficient real world examples for training. To provide access to bug fix datasets of this scale, we are releasing two datasets called SSB-9M and TSSB-3M. \nWhile SSB-9M provides access to a collection of over 9M general single statement bug fixes from over 500K open source Python projects , TSSB-3M focuses on over 3M single statement bugs which can be fixed solely by a single statement change. To facilitate future research and empirical investigations, we annotated each bug fix with one of 20 single statement bug (SStuB) patterns typical for Python together with a characterization of the code change as a sequence of AST modifications. Our initial investigation shows that at least 40% of all single statement bug fixes mined fit at least one SStuB pattern, and that the majority of 72% of all bugs can be fixed with the same syntactic modifications as needed for fixing SStuBs.","2022-05-23T00:08:52Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] TSSB-3M: Mining single statement bugs at massive scale - Cedric Richter, Heike Wehrheim",""
"2022-05-23T15:54:00Z","2022-05-23T16:02:00Z","2024-03-21T14:46:32Z","","91aa8c43-5591-4acd-94c0-2618201ed813@conf.researchr.org","","2022-05-13T04:44:54Z","This paper presents LAGOON – an open source platform for understanding the complex ecosystems of Open Source Software (OSS) communities. The platform currently utilizes spatiotemporal graphs to store and investigate the artifacts produced by these communities, and help analysts identify bad actors who might compromise an OSS project’s security. LAGOON provides ingest of artifacts from several common sources, including source code repositories, issue trackers, mailing lists and scraping content from project websites. Ingestion utilizes a modular architecture, which supports incremental updates from data sources and provides a generic identity fusion process that can recognize the same community members across disparate accounts. A user interface is provided for visualization and exploration of an OSS project’s complete sociotechnical graph. Scripts are provided for applying machine learning to identify patterns within the data. While current focus is on the identification of bad actors in the Python community, the platform’s reusability makes it easily extensible with new data and analyses, paving the way for LAGOON to become a comprehensive means of assessing various OSS-based projects and their communities.","2022-05-23T00:09:18Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] LAGOON: An Analysis Tool for Open Source Communities - Sourya Dey, Walt Woods",""
"2022-05-23T16:02:00Z","2022-05-23T16:10:00Z","2024-03-21T14:46:32Z","","e53f2c60-6b02-4712-a174-6b999fda536c@conf.researchr.org","","2022-05-13T04:44:54Z","Phabricator is a modern code collaboration tool used by popular projects like FreeBSD and Mozilla. However, unlike the other well-known code review environments, such as Gerrit or GitHub, there does not exist a readily accessible public dataset of Phabricator. This paper describes our experience mining code reviews from five different projects that use Phabricator (Blender, FreeBSD, KDE, LLVM, and Mozilla) and discusses the problems associated with the data retrieval process. We publish a dataset with details of 317,476 code reviews conducted via Phabricator. Our dataset is available in both JSON and MySQL database dump formats. The dataset enables analyses of the history of code reviews at a finer granular level than is possible on the other platforms. In addition, given that the projects we mined are accessible via the Conduit API, our dataset can be used as a foundation to fetch additional details and insights.","2022-05-23T00:12:20Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] The Unexplored Treasure Trove of Phabricator Code Reviews - Gunnar Kudrjavets, Nachiappan Nagappan, Ayushi Rastogi",""
"2022-05-23T16:10:00Z","2022-05-23T16:30:00Z","2024-03-21T14:46:32Z","","c81a6a2b-0308-4f58-83d0-8c73573592f7@conf.researchr.org","","2022-05-14T08:53:41Z","","2022-05-23T00:12:29Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-23T17:30:00Z","2022-05-23T17:45:00Z","2024-03-21T14:46:32Z","","9522c428-17c5-40ec-ae54-362c2fd40549@conf.researchr.org","","2022-05-14T04:57:34Z","Despite decades of research, SE lacks widely accepted models (that offer precise quantitative stable predictions) about what factors most influence software quality. This paper provides a promising result showing such stable models can be generated using a new transfer learning framework called “STABILIZER”. Given a tree of recursively clustered projects (using project meta-data), STABILIZER promotes a model upwards if it performs best in the lower clusters (stopping when the promoted model performs worse than the models seen at a lower level). \nThe number of models found by STABILIZER is minimal: one for defect prediction (756 projects) and less than a dozen for project health (1628 projects). Hence, via STABILIZER, it is possible to find a few projects which can be used for transfer learning and make conclusions that hold across hundreds of projects at a time. Further, the models produced in this manner offer predictions that perform as well or better than the prior state-of-the-art. \nTo the best of our knowledge, STABILIZER is the order of magnitude faster than the prior state-of-the-art transfer learners which seek to find conclusion stability, and these case studies are the largest demonstration of the generalizability of quantitative predictions of project quality yet reported in the SE literature. \nIn order to support open science, all our scripts and data are online at https://github.com/Anonymous633671/STABILIZER.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Methods for Stabilizing Models across Large Samples of Projects(with case studies on Predicting Defect and Project Health) - Suvodeep Majumder, Tianpei Xia, Rahul Krishna, Tim Menzies",""
"2022-05-23T17:45:00Z","2022-05-23T18:00:00Z","2024-03-21T14:46:32Z","","7904ac97-f424-49f4-940a-0dfe1fbfa11f@conf.researchr.org","","2022-05-14T04:57:34Z","Code embedding is a keystone in the application of machine learning on several Software Engineering (SE) tasks. To effectively support a plethora of SE tasks, the embedding needs to capture program syntax and semantics in a way that is generic. To this end, we propose the first self-supervised pre-training approach (called GraphCode2Vec) which produces task-agnostic embedding of lexical and program dependence features. GraphCode2Vec achieves this via a synergistic combination of code analysis and Graph Neural Networks. GraphCode2Vec is generic, it allows pre-training, and it is applicable to several SE downstream tasks. We evaluate the effectiveness of GraphCode2Vec on four (4) tasks (method name prediction, solution classification, mutation testing and overfitted patch classification), and compare it with four (4) similarly generic code embedding baselines (Code2Seq, Code2Vec, CodeBERT, GraphCodeBERT) and 7 task-specific, learning-based methods. In particular, GraphCode2Vec is more effective than both generic and task-specific learning-based baselines. It is also complementary and comparable to GraphCodeBERT (a larger and more complex model). We also demonstrate through a probing and ablation study that GraphCode2Vec learns lexical and program dependence features and that self-supervised pre-training improves effectiveness.","2022-05-14T09:37:14Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses - Wei Ma, Mengjie Zhao, Ezekiel Soremekun, Qiang Hu, Jie M. Zhang, Mike Papadakis, Maxime Cordy, Xiaofei Xie, Yves Le Traon",""
"2022-05-23T18:00:00Z","2022-05-23T18:15:00Z","2024-03-21T14:46:32Z","","e58aed90-fc85-4cf7-ab23-8cab6e491ed3@conf.researchr.org","","2022-05-14T04:57:34Z","Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with \\emph{Senatus}, a new code-to-code recommendation engine. At the core of Senatus is \\emph{De-Skew} LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example on the CodeSearchNet dataset Senatus improves performance by 31.21% F1 and 147.9\\emph{x} faster query time compared to Facebook Aroma. Senatus also outperforms standard MinHash LSH by 29.2% F1 and 51.02\\emph{x} faster query time.","2022-05-14T09:37:21Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Senatus: A Fast and Accurate Code-to-Code Recommendation Engine - Fran Silavong, Sean Moran, Antonios Georgiadis, Rohan Saphal, Robert Otter",""
"2022-05-23T18:15:00Z","2022-05-23T18:23:00Z","2024-03-21T14:46:32Z","","4513f565-8eb3-4d49-b27a-5c5bbc12aef8@conf.researchr.org","","2022-05-14T04:57:34Z","An important function of code review is to increase understanding; helping reviewers understand a code change aides in knowledge transfer and finding bugs. Comments in code largely serve a similar purpose, helping future readers understand the program. It is thus natural to study what happens when these two forms of understanding collide. We ask: what documentation-related comments do reviewers make and how do they affect understanding of the contribution? We analyze ca.~700K review comments on 2,000 (Java and Python) GitHub projects, and propose several filters to identify which comments are likely to be either in response to a change in documentation and/or a call for such a change. We identify 65K such cases. We next develop a taxonomy of the reviewer intents behind such “comments on comments”. We find that achieving a shared understanding of the code is key: reviewer comments most often focused on clarification, followed by pointing out issues to fix, such as typos and outdated comments. Curiously, clarifying comments were frequently suggested (often verbatim) by the reviewer, indicating a desire to persist their understanding acquired during code review. We conclude with a discussion of implications of our comments-on-comments dataset for research on improving code review, including the potential benefits for automating code review.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Comments on Comments: Where Code Review and Documentation Meet - Nikitha Rao, Jason Tsay, Martin Hirzel, Vincent J. Hellendoorn",""
"2022-05-23T18:23:00Z","2022-05-23T18:31:00Z","2024-03-21T14:46:32Z","","bb113dce-3ba6-46c1-a069-6abc89377b21@conf.researchr.org","","2022-05-14T04:57:34Z","Compiler fuzzing tools such as Csmith have uncovered many bugs in compilers by randomly sampling programs from a generative model. The success of these tools is often attributed to their ability to generate unexpected corner case inputs that developers tend to overlook during manual testing. At the same time, their chaotic nature makes fuzzer-generated test cases notoriously hard to interpret, which has lead to the creation of input simplification tools such as C-Reduce (for C compiler bugs). In until now unrelated work, researchers have also shown that human-written software tends to be rather repetitive and predictable to language models. Studies show that developers deliberately write more predictable code, whereas code with bugs is relatively unpredictable. In this study, we ask the natural questions of whether this high predictability property of code also, and perhaps counter-intuitively, applies to fuzzer-generated code. That is, we investigate whether fuzzer-generated compiler inputs are deemed unpredictable by a language model built on human-written code and surprisingly conclude that \\emph{it is not}. To the contrary, Csmith fuzzer-generated programs are \\emph{more} predictable on a per-token basis than human-written C programs. Furthermore, bug-triggering tended to be more predictable still than random inputs, and the C-Reduce minimization tool did not substantially increase this predictability. Rather, we find that bug-triggering inputs are unpredictable relative to \\emph{Csmith’s own} generative model. This is encouraging; our results suggest promising research directions on incorporating predictability metrics in the fuzzing and reduction tools themselves.","2022-05-14T09:37:34Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] On the Naturalness of Fuzzer Generated Code - Rajeswari Hita Kambhamettu, John Billos, Carolyn Tomi Oluwaseun-Apo, Benjamin Gafford, Rohan Padhye, Vincent J. Hellendoorn",""
"2022-05-23T18:31:00Z","2022-05-23T18:39:00Z","2024-03-21T14:46:32Z","","e70bda68-eef4-4b7a-8ac1-0444f36274a3@conf.researchr.org","","2022-05-14T04:57:34Z","Stack Overflow (SO) is becoming an indispensable part of the modern software development workflow. However, navigating SO posts and comparing different solutions is time-consuming and cumbersome given the limited time, attention, and memory capacity of programmers. Recent research has proposed to summarize SO posts to concise text to help programmers quickly decide the relevance and quality of SO posts. Yet there is no large, comprehensive dataset of high-quality SO post summaries, which hinders the development and evaluation of post summarization techniques. We present SOSum, a dataset of 2278 popular SO posts with manually labeled summative sentences. Questions in SOSum cover 669 tags with a median view count of 253K and a median post score of 17. This dataset will foster research on sentence-level summarization of SO posts and has the potential to facilitate text summarization research on other types of textual software artifacts such as programming tutorials.","2022-05-14T09:37:43Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] SOSum: A Dataset of Stack Overflow Post Summaries - Bonan Kou, Yifeng Di, Muhao Chen, Tianyi Zhang",""
"2022-05-23T18:39:00Z","2022-05-23T19:00:00Z","2024-03-21T14:46:32Z","","14f7d6bc-56af-4869-8c75-355673c7682e@conf.researchr.org","","2022-05-14T09:07:10Z","","2022-05-14T09:49:00Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-24T13:00:00Z","2022-05-24T13:15:00Z","2024-03-21T14:46:32Z","","d4f28e9f-baa9-487b-848b-a28ef651b0ef@conf.researchr.org","","2022-05-13T04:48:54Z","Infrastructure as Code is the practice of automating the provisioning, configuration, and orchestration of network nodes using code in which variable values such as configuration parameters, node hostnames, etc. play a central role. Mistakes in these values are an important cause of infrastructure defects and corresponding outages. Ansible, a popular IaC language, nonetheless features semantics which can cause confusion about the value of variables. \nIn this paper, we identify six novel code smells related to Ansible’s intricate variable precedence rules and lazy-evaluated template expressions. Their detection requires an accurate representation of control and data flow, for which we transpose the program dependence graph to Ansible. We use the resulting detector to empirically investigate the prevalence of these variable smells in 21,931 open-source Ansible roles, uncovering 31,334 unique smell instances across 4,260 roles. We observe an upward trend in the number of variable smells over time, that it may take a long time before they are fixed, and that code changes more often introduce new smells than fix existing ones. Our results are a call to arms for more in-depth quality checkers for IaC code, and highlight the importance of transcending syntax in IaC research.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Smelly Variables in Ansible Infrastructure Code: Detection, Prevalence, and Lifetime - Ruben Opdebeeck, Ahmed Zerouali, Coen De Roover",""
"2022-05-24T13:15:00Z","2022-05-24T13:30:00Z","2024-03-21T14:46:32Z","","26c6771f-84fc-4b4b-bb51-903faec09f70@conf.researchr.org","","2022-05-13T04:48:54Z","Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, and Subtask. While previous research has focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. \nFor this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal / Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, as expected, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal / Causal links. \nMotivated by the differences between the types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on our JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Beyond Duplicates: Towards Understanding and Predicting Link Types in Issue Tracking Systems - Clara Marie Lüders, Abir Bouraffa, Walid Maalej",""
"2022-05-24T13:30:00Z","2022-05-24T13:45:00Z","2024-03-21T14:46:32Z","","ec5159ef-4af2-4882-bb42-7e67aa3fef40@conf.researchr.org","","2022-05-13T04:48:54Z","To reduce technical debt and make code more maintainable, it is important to be able to warn programmers about code smells. State-of-the-art code small detectors use deep learners, without much exploration of alternatives within that technology. \nOne promising alternative for software analytics and deep learning is “GHOST” that relies on a combination of hyper-parameter optimization of feedforward neural networks and a novel oversampling technique to deal with class imbalance. \nThe prior study from TSE’21 proposing this novel “fuzzy sampling” was somewhat limited in that the method was tested on defect prediction, but nothing else. Like defect prediction, code smell detection datasets have a class imbalance (which motivated “fuzzy sampling”). Hence, in this work we test if fuzzy sampling is useful for code smell detection. \nThe results of this paper show that we can achieve better than state-of-the-art results on code smell detection with fuzzy oversampling. For example, for “feature envy”, we were able to achieve 99+% AUC across all our datasets, and on 8/10 datasets for “misplaced class” While our specific results refer to code smell detection, they do suggest other lessons for other kinds of analytics. For example: (a) try better preprocessing before trying complex learners (b) include simpler learners as a baseline in software analytics (c) try “fuzzy sampling” as one such baseline.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] How to Improve Deep Learning for Software Analytics (a case study with code smell detection) - Rahul Yedida, Tim Menzies",""
"2022-05-24T13:45:00Z","2022-05-24T13:53:00Z","2024-03-21T14:46:32Z","","aa2382dd-6fdc-41f3-be45-33af826c7fcb@conf.researchr.org","","2022-05-13T04:48:54Z","The static properties of code repositories, e.g., lines of code, dependents, dependencies, etc. can be readily scraped from code hosting platforms such as GitHub, and from package management systems such as npm for JavaScript; Although no less important, information related to the \\textit{dynamic} properties of programs, e.g., number of tests in a test suite that pass or fail, is less readily available. The ability to easily collect this dynamic information could be immensely useful to researchers conducting corpus analyses, as they could differentiate projects based on properties that can only be observed by running them. \nIn this paper, we present npm-filter, an automated tool that can download, install, build, test, and run custom user scripts over the source code of JavaScript projects available on npm, the most popular JavaScript package manager. We outline this tool, describe its implementation, and show that npm-filter has already been useful in developing evaluation suites for multiple JavaScript tools.","2022-05-14T09:45:54Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] npm-filter: Automating the mining of dynamic information from npm packages - Ellen Arteca, Alexi Turcotte",""
"2022-05-24T13:53:00Z","2022-05-24T14:01:00Z","2024-03-21T14:46:32Z","","735c620a-eaeb-4253-948f-f8b10a8f267f@conf.researchr.org","","2022-05-13T04:48:54Z","To meet project timelines or budget constraints, developers intentionally deviate from writing optimal code to feasible code in what is known as incurring Technical Debt (TD). Furthermore, as part of planning their correction, developers document these deficiencies as comments in the code (i.e., self-admitted technical debt or SATD). As a means of improving source code quality, developers often apply a series of refactoring operations to their codebase. In this study, we explore developers repaying this debt through refactoring operations by examining occurrences of SATD removal in the code of 76 open-source Java systems. Our findings show that TD payment usually occurs with refactoring activities and developers refactor their code to remove TD for specific reasons. We envision our findings supporting vendors in providing tools to better support developers in the automatic repayment of technical debt.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Refactoring Debt: Myth or Reality? An Exploratory Study on the Relationship Between Technical Debt and Refactoring - Anthony Peruma, Eman Abdullah AlOmar, Christian D. Newman, Mohamed Wiem Mkaouer, Ali Ouni",""
"2022-05-24T14:01:00Z","2022-05-24T14:09:00Z","2024-03-21T14:46:32Z","","98d7e627-ee6c-4af8-9875-bfc86f02b9b0@conf.researchr.org","","2022-05-13T04:48:54Z","Context: Cryptographic APIs are often misused in real-world applications. Therefore, many cryptographic API misuse detection tools have been introduced. However, there exists no established reference benchmark for a fair and comprehensive comparison and evaluation of these tools. While there are benchmarks, they often only address a subset of the domain or were only used to evaluate a subset of existing misuse detection tools. Objective: To fairly compare cryptographic API misuse detection tools and to drive future development in this domain, we will devise such a benchmark. Openness and transparency in the generation process are key factors to fairly generate and establish the needed benchmark. Method:We propose an approach where we derive the benchmark generation methodology from the literature which consists of general best practices in benchmarking and domain-specific benchmark generation. A part of this methodology is transparency and openness of the generation process, which is achieved by pre-registering this work. Based on our methodology we design CamBench, a fair “Cryptographic API Misuse Detection Tool Benchmark Suite”. We will implement the first version of CamBench limiting the domain to Java, the JCA, and static analyses. Finally, we will use CamBench to compare current misuse detection tools and compare CamBench to related benchmarks of its domain.","2022-05-14T14:17:12Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] CamBench - Cryptographic API Misuse Detection Tool Benchmark Suite - Michael Schlichtig, Anna-Katharina Wickert, Stefan Krüger, Eric Bodden, Mira Mezini",""
"2022-05-24T14:09:00Z","2022-05-24T14:30:00Z","2024-03-21T14:46:32Z","","4c925ca4-0088-4c9a-a036-2e4379c10234@conf.researchr.org","","2022-05-14T09:42:59Z","","2022-05-14T09:49:00Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-24T15:00:00Z","2022-05-24T15:15:00Z","2024-03-21T14:46:32Z","","a18d1cab-fa5e-4a14-9a72-16c82fadcfc8@conf.researchr.org","","2022-05-13T04:50:04Z","Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged but at the expense of run-time performance. While hybrid approaches aim for the “best of both worlds,” the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges—and resultant bugs—involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation—the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.","2022-07-18T20:18:33Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study - Tatiana Castro Vélez, Raffi Khatchadourian, Mehdi Bagherzadeh, Anita Raja",""
"2022-05-24T15:15:00Z","2022-05-24T15:30:00Z","2024-03-21T14:46:32Z","","8fa59993-e526-49ed-978d-d760e0febf6e@conf.researchr.org","","2022-05-13T04:50:04Z","Quantitative studies on the border between Mining Software Repository (MSR) and Empirical Software Engineering (ESE) apply data analysis methods, like regression modeling, statistic tests or correlation analysis, to commits or pulls to better understand the software development process. Such studies assure the validity of the reported results by following a sound methodology. However, with increasing complexity, parts of the methodology can still go wrong. This may result in MSR/ESE studies with undetected threats to validity. In this paper, we propose to systematically protect against threats by operationalizing their treatment using simulations. A simulation substitutes observed and unobserved data, related to an MSR/ESE scenario, with synthetic data, carefully defined according to plausible assumptions on the scenario. Within a simulation, unobserved data becomes transparent, which is the key difference to a real study, necessary to detect threats to an analysis methodology. Running an analysis methodology on synthetic data may detect basic technical bugs and misinterpretations, but it also improves the trust in the methodology. The contribution of a simulation is to operationalize testing the impact of important assumptions. Assumptions still need to be rated for plausibility. We evaluate simulation-based testing by operationalizing undetected threats in the context of four published MSR/ESE studies. We recommend that future research uses such more systematic treatment of threats, as a contribution against the reproducibility crisis.","2022-05-21T02:43:43Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Operationalizing Threats to MSR Studies by Simulation-Based Testing - Johannes Härtel, Ralf Laemmel",""
"2022-05-24T15:30:00Z","2022-05-24T15:38:00Z","2024-03-21T14:46:32Z","","645d406a-a1fd-486b-8c7d-085e296bdd4b@conf.researchr.org","","2022-05-13T04:50:04Z","We conduct an exploratory, large-scale, longitudinal study of 50 years of commits to publicly available version control system repositories, in order to characterize the geographic diversity of contributors to public code and its evolution over time. We analyze in total 2.2 billion commits collected by Software Heritage from 160 million projects and authored by 43 million authors during the 1971-2021 time period. We geolocate developers to 12 world regions derived from the United Nation geoscheme, using as signals email top-level domains, author names compared with names distributions around the world, and UTC offsets mined from commit metadata. We find evidence of the early dominance of North America in open source software, later joined by Europe. After that period, the geographic diversity in public code has been constantly increasing. We also identify relevant historical shifts related to the UNIX wars, the increase of coding literacy in Central and South Asia, and broader phenomena like colonialism and people movement across countries (immigration/emigration).","2022-05-21T02:43:44Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Geographic Diversity in Public Code Contributions - Davide Rossi, Stefano Zacchiroli",""
"2022-05-24T15:38:00Z","2022-05-24T15:46:00Z","2024-03-21T14:46:32Z","","c124d07e-3a68-4651-aeee-8bd91116a8c9@conf.researchr.org","","2022-05-13T04:50:04Z","We introduce the General Index of Software Engineering Papers, a dataset of fulltext-indexed papers from the most prominent scientific venues in the field of Software Engineering. The dataset includes both complete bibliographic information and indexed n-grams (sequence of contiguous words after removal of stopwords and non-words, for a total of 577’276’382 unique n-grams in this release) with length 1 to 5 for 44’581 papers retrieved from 34 venues over the 1971–2020 period. The dataset serves use cases in the field of meta-research, allowing to introspect the output of software engineering research even when access to papers or scholarly search engines is not possible (e.g., due to contractual reasons). The dataset also contributes to making such analyses reproducible and independently verifiable, as opposed to what happens when they are conducted using 3rd-party and non-open scholarly indexing services. The dataset is available as a portable Postgres database dump and released as open data.","2022-05-21T08:53:09Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] The General Index of Software Engineering Papers - Zeinab Abou Khalil, Stefano Zacchiroli",""
"2022-05-24T15:46:00Z","2022-05-24T15:54:00Z","2024-03-21T14:46:32Z","","9155d2a2-4490-4623-8efc-061bead897b6@conf.researchr.org","","2022-05-13T04:50:04Z","Context: Forgetting is defined as a gradual process of losing information. Even though there are many studies demonstrating the effect of forgetting in software development, to the best of our knowledge, no study explores the impact of forgetting in software development using a controlled experiment approach. Objective: We would like to provide insights on the impact of forgetting in software development projects. We want to examine whether the recency &amp; frequency of interaction impact forgetting in software development. Methods: We will conduct an experiment that examines the impact of forgetting in software development. Participants will first do an initial task. According to their initial task performance, they will be assigned to either the experiment or the control group. The experiment group will then do two additional tasks to enhance their exposure to the code. Both groups will then do a final task to see if additional exposure to the code benefits the experiment group’s performance in the final task. Finally, we will conduct a survey and a recall task with the same participants to collect data about their perceptions of forgetting and quantify their memory performance, respectively","2022-05-21T08:53:33Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Investigating the Impact of Forgetting in Software Development - Utku Unal, Eray Tüzün, Tamer Gezici, Ausaf Ahmed Farooqui",""
"2022-05-24T15:54:00Z","2022-05-24T16:15:00Z","2024-03-21T14:46:32Z","","5e57f628-3c40-4d8c-a3e4-c5d81bea05e5@conf.researchr.org","","2022-05-14T09:47:05Z","","2022-05-21T08:53:41Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Discussions and Q&A",""
"2022-05-24T19:30:00Z","2022-05-24T19:45:00Z","2024-03-21T14:46:32Z","","509d5bc2-6c28-4d26-9248-4ce48bee8e48@conf.researchr.org","","2022-05-14T05:04:24Z","Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the codebase. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria in the context of MCR. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.","2022-05-14T09:49:37Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Code Review Practices for Refactoring Changes: An Empirical Study on OpenStack - Eman Abdullah AlOmar, Moataz Chouchen, Mohamed Wiem Mkaouer, Ali Ouni",""
"2022-05-24T19:45:00Z","2022-05-24T20:00:00Z","2024-03-21T14:46:32Z","","0a79b80e-d81b-40d7-94de-906d65feaf58@conf.researchr.org","","2022-05-14T05:04:24Z","The automotive industry has transitioned from being electro-mechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook’s front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing a first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing their characteristics and development styles. \nThe landscape is defined by 15,000+ users contributing to ~600 actively-developed automotive projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe disruptions along multiple dimensions, including shift in preferred language from MATLAB to Python and prevalence of perception and decision related software over traditional automotive software. This study witnesses open source automotive software boom in its infancy with huge potential and implications for future research and practice.","2022-05-14T09:49:46Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Painting the Landscape of Automotive Software in GitHub - Sangeeth Kochanthara, Yanja Dajsuren, Loek Cleophas, Mark van den Brand",""
"2022-05-24T20:00:00Z","2022-05-24T20:08:00Z","2024-03-21T14:46:32Z","","d7276cc9-2ced-4017-908c-ed093bb7a77d@conf.researchr.org","","2022-05-14T05:04:24Z","MATLAB/Simulink is widely used for model-based design. Engineers create Simulink models and compile them to embedded code, often to control safety-critical cyber-physical systems in automotive, aerospace, and healthcare applications. Despite Simulink’s importance, there are few large-scale empirical Simulink studies, perhaps because there is no large readily available corpus of third-party open-source Simulink models. To enable empirical Simulink studies, this paper introduces SLNET, the largest corpus of freely available third-party Simulink models. SLNET has several advantages over earlier collections. Specifically, SLNET is 8 times larger than the largest previous corpus of Simulink models, includes fine-grained metadata, is constructed automatically, is self-contained, and allows redistribution. SLNET is available under permissive open-source licenses and contains all of its collection and analysis tools.","2022-05-14T09:49:55Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] SLNET: A Redistributable Corpus of 3rd-party Simulink Models - Sohil Lal Shrestha, Shafiul Azam Chowdhury, Christoph Csallner",""
"2022-05-24T20:08:00Z","2022-05-24T20:16:00Z","2024-03-21T14:46:32Z","","388456c0-122e-4bcc-a0e8-44695ddd469c@conf.researchr.org","","2022-05-14T05:04:24Z","Numerous tools exist for mining source code and software development process metrics. However, very few publicly available tools focus on source code comments, a crucial software artifact. This paper presents SoCCMiner (Source Code-Comments and Comment-Context Miner), a tool that offers multiple mining pipelines. It is the first readily available (plug-and-play) and customizable open-source tool for mining source code contextual information of comments at different granularities (Class comments, Method comments, Interface comments, and other granular comments). Mining comments at different source code granularities can aid researchers and practitioners working in a host of applications that focus on source code comments, such as Self-Admitted Technical Debt, Program Comprehension, and other applications. Furthermore, it is highly adaptable and extendable to include additional attributes and support other programming languages. This prototype supports the Java programming language.","2022-05-14T09:50:02Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] SoCCMiner: A Source Code-Comments and Comment-Context Miner - Murali Sridharan, Mika Mäntylä, Maëlick Claes, Leevi Rantala",""
"2022-05-24T20:16:00Z","2022-05-24T20:24:00Z","2024-03-21T14:46:32Z","","38c0a34c-b0d3-4dba-9e7a-bac937046f9b@conf.researchr.org","","2022-05-14T05:04:24Z","Understanding the practice of refactoring documentation is of paramount importance in academia and industry. Issue tracking systems are used by most software projects enabling developers, quality assurance, managers, and users to submit feature requests and other tasks such as bug fixing and code review. Although recent studies explored how to document refactoring in commit messages, little is known about how developers describe their refactoring needs in issues. In this study, we aim at exploring developer-reported refactoring changes in issues to better understand what developers consider to be problematic in their code and how they handle it. Our approach relies on text mining 45,477 refactoring-related issues and identifying refactoring patterns from a diverse corpus of 77 Java projects by investigating issues associated with 15,833 refactoring operations and developers’ explicit refactoring intention. Our results show that (1) developers mostly use move refactoring related terms/phrases to target refactoring related issues; and (2) developers tend to explicitly mention the improvement of specific quality attributes and focus on duplicate code removal. We envision our findings enabling tool builders to support developers with automated documentation of refactoring changes in issues.","2022-05-14T09:50:07Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] An Exploratory Study on Refactoring Documentation in Issues Handling - Eman Abdullah AlOmar, Anthony Peruma, Mohamed Wiem Mkaouer, Christian D. Newman, Ali Ouni",""
"2022-05-24T20:24:00Z","2022-05-24T20:32:00Z","2024-03-21T14:46:32Z","","fe1ea938-9e23-4d51-9d21-836e832385e8@conf.researchr.org","","2022-05-14T05:04:24Z","In recent years many Open-Source Software (OSS) projects have adopted various automations to automate repetitive tasks, one category of automations adopted by OSS are so-called bots. In previous work, researchers have found that the adoption of bots helps open-source developers merge more pull requests and reduces the need for communication between developers. The Apache Software Foundation (ASF) is a foundation that provides open-source software, and it supports the OSS projects that are a member of it. Projects that are a part of the ASF can choose to adopt the ASFBot, this bot automatically creates links between the JIRA issue tracker and pull requests (PRs) on GitHub. In this exploratory case study, we zoom in on the ASF ecosystem, and we seek to understand how the adoption of one specific bot (the ASFBot) impacts the discussions in the issue-trackers of these projects. In this study, we use the SmartShark dataset to investigate whether the ASFBot affects (i)human comments mentioning pull requests (PRs) and fixes in issue comments and (ii) the general human comment rate on issues. We apply a regression discontinuity design (RDD) on nine projects that are members of the ASF and which have been active both before and after the adoption of the ASFBot. Our results indicate (i) a decrease in comments mentioning pull requests and fixes after the bot adoption and (ii) no effect in the number of human comments after the bot adoption. By taking a first step towards understanding how the adoption of ASFBot impacts the issue tracker of projects we can better understand the advantages that the infrastructure of a foundation like ASF provides, and how it affects the commenting behavior of developers in the issue-tracker.","2022-05-14T09:50:11Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Between JIRA and GitHub: ASFBot and its Influence on Human Comments in Issue Trackers - Ambarish Moharil, Dmitrii Orlov, Samar Jameel, Tristan Trouwen, Nathan Cassee, Alexander Serebrenik",""
"2022-05-24T20:32:00Z","2022-05-24T21:00:00Z","2024-03-21T14:46:32Z","","c7525d81-31c6-4ebb-81f0-efcc400ce7e0@conf.researchr.org","","2022-05-14T09:49:00Z","","2022-05-14T09:50:17Z","Room 315+316 - 1000 Fort Duquesne Boulevard, Pittsburgh, United States","","","[MSR Technical Papers] Discussions and Q&A",""